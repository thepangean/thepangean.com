<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="/tag/society/feed.xml" rel="self" type="application/atom+xml" />
  <link href="/" rel="alternate" type="text/html" />
  <updated>2021-03-24T09:48:24+00:00</updated>
  <id>/tag/society/feed.xml</id>

  
  
  

  
    <title type="html">The Pangean | </title>
  

  
    <subtitle>Taking a step back</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">Rise of the New Age Nazis</title>
      <link href="/Rise-of-the-New-Age-Nazis" rel="alternate" type="text/html" title="Rise of the New Age Nazis" />
      <published>2021-03-23T00:00:00+00:00</published>
      <updated>2021-03-23T00:00:00+00:00</updated>
      <id>/Rise-of-the-New-Age-Nazis</id>
      <content type="html" xml:base="/Rise-of-the-New-Age-Nazis">&lt;p&gt;On December 14th, 2020, the Labour leader Keir Starmer appeared on radio show LBC with host Nick Ferrari when a caller, introducing herself as ‘Gemma from Cambridge,’ took the opportunity to propagate the Neo — Nazi ‘Great Replacement’ conspiracy theory on a national media platform. She stated: “in the wake of organisations such as BLM and other racial advocacy groups pushing what’s best for their people, I just want to ask, should white people also start playing identity politics now, before they become a minority themselves by 2066?”&lt;/p&gt;
&lt;p&gt;This bizarre event was made even more bewildering when a Twitter user &lt;a href=&quot;https://twitter.com/redflareinfo/status/1338453892661714945&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;revealed&lt;/a&gt; that ‘Gemma from Cambridge’ was in fact Jody Swingler, a yoga teacher and musician living in Ibiza, a far cry from the archetypal image of the hapless neo-nazi social outcast in their parents’ basement, pulling the wings off flies and ranting on obscure online messageboards about how The Jews had prevented them from securing a girlfriend. Swingler had also &lt;a href=&quot;https://twitter.com/redflareinfo/status/1338453897258668033&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;recorded&lt;/a&gt; two YouTube shows with Mark Collett and Laura Melia, leaders of the white nationalist political party Patriotic Alternative, which &lt;a href=&quot;https://www.hopenothate.org.uk/wp-content/uploads/2020/08/HnH_Patriotic-Alternative-report_2020-08-v3.pdf&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;opposes&lt;/a&gt; the “replacement and displacement” of white Britons by people who “have no right to these lands.”&lt;/p&gt;
&lt;p&gt;It may seem striking that people who advocate new age, spiritual beliefs, previously associated with ‘flower power’ and the countercultural movement of the 1960s, could wind up on the same side as the far right. But in fact, the link between new age spirituality and extremist politics is not new. The Nazis drew support from the occultist Thule Society, and prominent Nazi figures such as Heinrich Himmler and Rudolf Hess endorsed homeopathy and alternative medicine, with Himmler supporting using plant extracts to cure cancer. In 1934, Hess set up an alternative medicine centre in Dresden. Their embrace of holism and spirituality, broadly, was underpinned by a rejection of the precepts of the Enlightenment, which posited that the world could be understood through logical processes, subordinating religious and monarchical obscurantism to the supremacy of the faculty of reason. Many Nazi figures saw a romanticised, agrarian age as a solution to the modern menaces of industrialisation and materialism. For example, Ernst Lehmann, a Nazi professor of botany, stated:&lt;/p&gt;
&lt;p&gt;“We recognize that separating humanity from nature, from the whole of life, leads to humankind’s own destruction and to the death of nations. Only through a reintegration of humanity into the whole of nature can our people be made stronger… This striving toward connectedness with the totality of life, with nature itself, a nature into which we are born, this is the deepest meaning and the true essence of National Socialist thought.”&lt;/p&gt;
&lt;p&gt;A desire to undo the crude, materialist logic that undergirded liberal democracy, and to restructure the relationship between the citizen and the State, therefore, spawned the Nazis’ fascination with holism and the reintegration of man with nature. “Blood and soil” meant an organic connection with one’s homeland that could not be expressed through the civic or legal frameworks engendered by the minority protection clauses of the League of Nations, but only through the maintenance of racial purity. Mark Mazower wrote in his book ‘Dark Continent’ that:“The League (of Nations) , after all, was an organisation of States. But what was the State? According to Hitler’s biological view of politics, it was no less than a ‘living organism,’” adding: “Hitler’s own vision of geopolitics unlike that of many geopoliticians — rested upon race: the State itself was merely an expression of the racial ‘Volk.’” Railing against the ‘juridification of politics,’ many Nazi legal theorists saw the nation as a biological organism, corruptible by outside influences and requiring protection from alleged Jewish subversion.&lt;/p&gt;
&lt;p&gt;What Marquette University history professor Peter Staudenmeier &lt;a href=&quot;https://newrepublic.com/article/154971/rise-ecofascism-history-white-nationalism-environmental-preservation-immigration&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;describes&lt;/a&gt; as the “link between a yearning for purity in the environmental sphere and a desire for racialized purity in the social sphere” also undergirds the ecofascist tendencies common to many modern neo-nazis such as Anders Brevik and the El Paso shooter, who characterised non white populations as invaders seeking to despoil the environment through having more children and consuming more resources. This was also reflected in the writings of ecologist Garrett Hardin, &lt;a href=&quot;https://www.splcenter.org/fighting-hate/extremist-files/individual/garrett-hardin&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;listed&lt;/a&gt; by the SPLC as a white nationalist, who wrote about the supposed threats that overpopulation posed to the Earth’s future. The myth of overpopulation has, however, been roundly debunked; a &lt;a href=&quot;https://www.theguardian.com/environment/2020/sep/21/worlds-richest-1-cause-double-co2-emissions-of-poorest-50-says-oxfam#:~:text=1%20month%20old-,World's%20richest%201%25%20cause%20double%20CO2,of%20poorest%2050%25%2C%20says%20Oxfam&amp;amp;text=The%20wealthiest%201%25%20of%20the,2015%2C%20according%20to%20new%20research.&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;study&lt;/a&gt; from Oxfam showed that the world’s richest 1% are responsible for double the CO2 emissions of the poorest 50%.&lt;/p&gt;
&lt;p&gt;More recently, the contention that COVID-19 is a hoax has been a point of convergence between new age hippies and the far-right. Back in October, thousands of anti-vaxxers marched through Trafalgar Square in London at the COVID-sceptic ‘Unite for Freedom’ event, &lt;a href=&quot;https://twitter.com/nicktolhurst/status/1299729414767497219?lang=en&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;during which&lt;/a&gt; a BUF (British Union of Fascists) flag was spotted. The event was hosted by notorious conspiracy theorist David Icke, whose theories have been endorsed by both neo-nazis groups such as Combat 18, and the new age spiritual movement. It was also &lt;a href=&quot;https://www.independent.co.uk/news/world/americas/us-election-2020/jake-angeli-qanon-shaman-stormed-capitol-b1784091.html&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;revealed&lt;/a&gt; that Jake Angeli, a Trump supporter nicknamed the ‘QAnon shaman’ who stormed the Capitol, had stated that COVID-19 was a hoax. According to his mother, he had also refused to eat non-organic food.&lt;/p&gt;
&lt;p&gt;The two groups are united in their rejection of the perceived infringement on their liberties; in the case of the far-right, the supposed stifling of their ability to criticise the unaccountable technocracy that is allegedly imposing open borders and multiculturalism, and in the case of the new age conspiracists, the personal, bodily autonomy connoted by “natural” methods of healing, which are at odds with modern, scientific forms of inoculation such as vaccines. The excoriation of Bill Gates as a central figure in a supposed global plot to undermine civil liberties by using vaccines as a method of social control, including implanting microchips into unfortunate victims, has also been &lt;a href=&quot;https://www.dailymotion.com/video/x64vj4&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;propagated&lt;/a&gt; by conspiracy theorist Alex Jones, who has suggested that vaccines are part of a government- induced eugenics programme.&lt;/p&gt;
&lt;p&gt;A theory known as the ‘Great Reset’ has also taken root in many far right circles, pushed by websites such as Breitbart, which asserts that COVID-19 represents an attempt by a cabal of wealthy politicians, financiers and bureaucrats to establish a global government, eroding national sovereignty and dictating fiscal and monetary policy. During a &lt;a href=&quot;https://www.youtube.com/watch?v=ugRnjpXEwTo&amp;amp;t=114s&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;conversation&lt;/a&gt; with former Alex Jones acolyte Paul Joseph Watson, Breitbart columnist James Delingpole described the Great Reset as “another variation on the theme of the New World Order,” stating: “it’s a technocratic elite — an unelected technocratic elite — deciding how you and I should live our lives.”&lt;/p&gt;
&lt;p&gt;Scepticism towards authority and heightened awareness of the erosion of civil liberties by governments using crises as a pretext to usher in technocratic dominance is not without merit. In an era characterised by the technocratic monopolisation of communications by companies whose business models are predicated on mining consumer data, in which a global pandemic has drastically increased the purview of the State’s influence over the lives of ordinary citizens, and in which finance, industry and production have all been centralised in the hands of a few multinational corporations, it is not unreasonable to be wary of authoritarian overreach.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;However, the vague criticisms of systems of power enunciated by new age hippies, and the wild, conspiratorial denunciations of the ‘New World Order’ or the ‘Great Reset,’ common to extreme right wing political figures lack an empirical, material analysis. While it is certainly true that in Britain, at least, the pandemic has seen a worrying development in outsourcing the ‘Test and Trace’ system to companies such as G4S and Serco, which have been involved in nefarious operations from &lt;a href=&quot;https://www.bbc.co.uk/news/uk-england-sussex-51573510&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;setting up&lt;/a&gt; immigration detention centres to &lt;a href=&quot;https://www.justice.gov.uk/about/hmps/contracted-out&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;developing&lt;/a&gt; some of Britain’s first for-profit prisons, the potential for government overreach is no grounds for denying the existence, or the severity, of COVID-19.&lt;/p&gt;
&lt;p&gt;As outlined above, both aforementioned groups reject a materialist, empirically rigorous analysis of the global economy and the role of the State in abrogating civil liberties, idealising a preindustrial, agrarian past and asserting that the world’s problems are caused by a shadowy cabal (the root of most antisemitic conspiracy theories). This pandemic has shown that these groups, though widely considered to have diametrically opposed political interests, are more similar than they appear, and have been willing to set aside their ideological differences for the pursuit of wider goals. Unless their scepticism of the technocratic impulses of governments seeking to curtail civil liberties, much of which is justified, can be countered with a thorough and precise refutation of conspiratorial political narratives, the implications of this unholy alliance could be significant.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Tom Perrett</name>
        
        
      </author>

      

      
        <category term="society" />
      

      
        <summary type="html">On December 14th, 2020, the Labour leader Keir Starmer appeared on radio show LBC with host Nick Ferrari when a caller, introducing herself as ‘Gemma from Cambridge,’ took the opportunity to propagate the Neo — Nazi ‘Great Replacement’ conspiracy theory on a national media platform. She stated: “in the wake of organisations such as BLM and other racial advocacy groups pushing what’s best for their people, I just want to ask, should white people also start playing identity politics now, before they become a minority themselves by 2066?” This bizarre event was made even more bewildering when a Twitter user revealed that ‘Gemma from Cambridge’ was in fact Jody Swingler, a yoga teacher and musician living in Ibiza, a far cry from the archetypal image of the hapless neo-nazi social outcast in their parents’ basement, pulling the wings off flies and ranting on obscure online messageboards about how The Jews had prevented them from securing a girlfriend. Swingler had also recorded two YouTube shows with Mark Collett and Laura Melia, leaders of the white nationalist political party Patriotic Alternative, which opposes the “replacement and displacement” of white Britons by people who “have no right to these lands.” It may seem striking that people who advocate new age, spiritual beliefs, previously associated with ‘flower power’ and the countercultural movement of the 1960s, could wind up on the same side as the far right. But in fact, the link between new age spirituality and extremist politics is not new. The Nazis drew support from the occultist Thule Society, and prominent Nazi figures such as Heinrich Himmler and Rudolf Hess endorsed homeopathy and alternative medicine, with Himmler supporting using plant extracts to cure cancer. In 1934, Hess set up an alternative medicine centre in Dresden. Their embrace of holism and spirituality, broadly, was underpinned by a rejection of the precepts of the Enlightenment, which posited that the world could be understood through logical processes, subordinating religious and monarchical obscurantism to the supremacy of the faculty of reason. Many Nazi figures saw a romanticised, agrarian age as a solution to the modern menaces of industrialisation and materialism. For example, Ernst Lehmann, a Nazi professor of botany, stated: “We recognize that separating humanity from nature, from the whole of life, leads to humankind’s own destruction and to the death of nations. Only through a reintegration of humanity into the whole of nature can our people be made stronger… This striving toward connectedness with the totality of life, with nature itself, a nature into which we are born, this is the deepest meaning and the true essence of National Socialist thought.” A desire to undo the crude, materialist logic that undergirded liberal democracy, and to restructure the relationship between the citizen and the State, therefore, spawned the Nazis’ fascination with holism and the reintegration of man with nature. “Blood and soil” meant an organic connection with one’s homeland that could not be expressed through the civic or legal frameworks engendered by the minority protection clauses of the League of Nations, but only through the maintenance of racial purity. Mark Mazower wrote in his book ‘Dark Continent’ that:“The League (of Nations) , after all, was an organisation of States. But what was the State? According to Hitler’s biological view of politics, it was no less than a ‘living organism,’” adding: “Hitler’s own vision of geopolitics unlike that of many geopoliticians — rested upon race: the State itself was merely an expression of the racial ‘Volk.’” Railing against the ‘juridification of politics,’ many Nazi legal theorists saw the nation as a biological organism, corruptible by outside influences and requiring protection from alleged Jewish subversion. What Marquette University history professor Peter Staudenmeier describes as the “link between a yearning for purity in the environmental sphere and a desire for racialized purity in the social sphere” also undergirds the ecofascist tendencies common to many modern neo-nazis such as Anders Brevik and the El Paso shooter, who characterised non white populations as invaders seeking to despoil the environment through having more children and consuming more resources. This was also reflected in the writings of ecologist Garrett Hardin, listed by the SPLC as a white nationalist, who wrote about the supposed threats that overpopulation posed to the Earth’s future. The myth of overpopulation has, however, been roundly debunked; a study from Oxfam showed that the world’s richest 1% are responsible for double the CO2 emissions of the poorest 50%. More recently, the contention that COVID-19 is a hoax has been a point of convergence between new age hippies and the far-right. Back in October, thousands of anti-vaxxers marched through Trafalgar Square in London at the COVID-sceptic ‘Unite for Freedom’ event, during which a BUF (British Union of Fascists) flag was spotted. The event was hosted by notorious conspiracy theorist David Icke, whose theories have been endorsed by both neo-nazis groups such as Combat 18, and the new age spiritual movement. It was also revealed that Jake Angeli, a Trump supporter nicknamed the ‘QAnon shaman’ who stormed the Capitol, had stated that COVID-19 was a hoax. According to his mother, he had also refused to eat non-organic food. The two groups are united in their rejection of the perceived infringement on their liberties; in the case of the far-right, the supposed stifling of their ability to criticise the unaccountable technocracy that is allegedly imposing open borders and multiculturalism, and in the case of the new age conspiracists, the personal, bodily autonomy connoted by “natural” methods of healing, which are at odds with modern, scientific forms of inoculation such as vaccines. The excoriation of Bill Gates as a central figure in a supposed global plot to undermine civil liberties by using vaccines as a method of social control, including implanting microchips into unfortunate victims, has also been propagated by conspiracy theorist Alex Jones, who has suggested that vaccines are part of a government- induced eugenics programme. A theory known as the ‘Great Reset’ has also taken root in many far right circles, pushed by websites such as Breitbart, which asserts that COVID-19 represents an attempt by a cabal of wealthy politicians, financiers and bureaucrats to establish a global government, eroding national sovereignty and dictating fiscal and monetary policy. During a conversation with former Alex Jones acolyte Paul Joseph Watson, Breitbart columnist James Delingpole described the Great Reset as “another variation on the theme of the New World Order,” stating: “it’s a technocratic elite — an unelected technocratic elite — deciding how you and I should live our lives.” Scepticism towards authority and heightened awareness of the erosion of civil liberties by governments using crises as a pretext to usher in technocratic dominance is not without merit. In an era characterised by the technocratic monopolisation of communications by companies whose business models are predicated on mining consumer data, in which a global pandemic has drastically increased the purview of the State’s influence over the lives of ordinary citizens, and in which finance, industry and production have all been centralised in the hands of a few multinational corporations, it is not unreasonable to be wary of authoritarian overreach.&amp;nbsp; However, the vague criticisms of systems of power enunciated by new age hippies, and the wild, conspiratorial denunciations of the ‘New World Order’ or the ‘Great Reset,’ common to extreme right wing political figures lack an empirical, material analysis. While it is certainly true that in Britain, at least, the pandemic has seen a worrying development in outsourcing the ‘Test and Trace’ system to companies such as G4S and Serco, which have been involved in nefarious operations from setting up immigration detention centres to developing some of Britain’s first for-profit prisons, the potential for government overreach is no grounds for denying the existence, or the severity, of COVID-19. As outlined above, both aforementioned groups reject a materialist, empirically rigorous analysis of the global economy and the role of the State in abrogating civil liberties, idealising a preindustrial, agrarian past and asserting that the world’s problems are caused by a shadowy cabal (the root of most antisemitic conspiracy theories). This pandemic has shown that these groups, though widely considered to have diametrically opposed political interests, are more similar than they appear, and have been willing to set aside their ideological differences for the pursuit of wider goals. Unless their scepticism of the technocratic impulses of governments seeking to curtail civil liberties, much of which is justified, can be countered with a thorough and precise refutation of conspiratorial political narratives, the implications of this unholy alliance could be significant.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Relax, AI is Not as Smart as You Think</title>
      <link href="/Relax-AI-is-Not-as-Smart-as-You-Think" rel="alternate" type="text/html" title="Relax, AI is Not as Smart as You Think" />
      <published>2021-03-23T00:00:00+00:00</published>
      <updated>2021-03-23T00:00:00+00:00</updated>
      <id>/Relax-AI-is-Not-as-Smart-as-You-Think</id>
      <content type="html" xml:base="/Relax-AI-is-Not-as-Smart-as-You-Think">&lt;p&gt;Artificial Intelligence (AI) is a term broadly used to describe some innovation that is trying to solve yet another problem. A &lt;a href=&quot;https://www.gov.uk/government/publications/artificial-intelligence-public-awareness-survey&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;survey&lt;/a&gt; by the UK government in 2019 on over 2,400 individuals reported that a good majority know what AI is, but only 12% know a lot about AI. This is no surprise, and the media doesn’t do well to clarify this uncertainty.&lt;/p&gt;
&lt;p&gt;An early definition of AI was proposed by Alan Turing 70 years ago. He described a method to determine if a machine exhibits intelligent behaviour, known as the 'Imitation Game' - a game where an interrogator judges if the responses to their questions are from a human or machine. Unsurprisingly, this is a great definition. However, as technology advances, problems that were once thought of as only solvable by humans have become ‘easily’ solvable by AI, and are no longer classified as AI problems. This paradox is known as the 'AI Effect'.&lt;/p&gt;
&lt;p&gt;Before breaking down some AI that you might have heard of, I want to be clear about something that is very often misunderstood. AI can be grouped into two categories: Logic Programming&lt;em&gt; &lt;/em&gt;(rule-based intelligence) and Machine Learning&lt;em&gt; &lt;/em&gt;(data-driven intelligence). In the former category, the intelligence of the AI is developed by researchers. They would very carefully study the domain of the problem and describe a set of rules and instructions which would best output a solution to the problem. In the latter category, intelligence is learned by AI. The researcher would typically use an existing model for learning (with tweakable settings) and apply it to historical data from the domain of the problem. Then the settings of the model which would output the best possible solution to the problem are found based on the data. The model with the optimal settings can then solve the problem.&lt;/p&gt;
&lt;p&gt;AI in both these categories has garnered a great deal of positive media attention, and rightfully so, but they have attributed to the great misunderstanding of AI that is widespread in society. Two examples of board game matches from the past 25 years apparently exhibit great strides in AI, but are mostly a testament to computing power rather than intelligent behaviour.&lt;/p&gt;
&lt;p&gt;A Logic Programming AI made history in 1997 when IBM’s supercomputer Deep Blue beat the then reigning world champion in a match of chess under standard tournament conditions. Deep Blue follows a set of logical rules to search for the best sequence of moves needed to win, with its prowess coming from its computational speed. At the time, it could evaluate about 200 million chess moves per second (7-12 turns ahead) and its strategy to win could be completely analysed and interpreted by the researchers, i.e. the reason why it made a particular move was fully understood. Although Deep Blue has no practical applications other than playing chess, it proved that AI could outperform human intelligence at a game often associated with requiring high intelligence.&lt;/p&gt;
&lt;p&gt;On a similar problem, a Machine Learning AI made history in 2016 when Google’s AlphaGo beat the 18-time world champion in a match of Go. Go is a far more complicated game than chess and it is arguably the most complex game ever devised. The number of possible moves from each position of a chess game is about 20 whereas a Go game is about 200. A Logic Programming AI would need to evaluate about 60 trillion moves to think just 6 turns ahead, so finding the best sequence of moves needed to win the game would be unfeasible. Google’s researchers used a model called Deep Reinforcement Learning to tackle the problem. This model of learning was developed in the 1980's and stemmed from research into animal psychology. In this case, the AI was initially presented with 100,000 games of Go and, given rewards when it won, it learned how to play the game. It then played against itself millions of times to iteratively get better, until it was better than a world champion Go player. Unfortunately, this intelligent behaviour comes at a cost; it took 3 days to train AlphaGo and it consumed what would usually be $&lt;a href=&quot;https://www.yuzeh.com/data/agz-cost.html&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;2 million&lt;/a&gt; of Google's computational power.&lt;/p&gt;
&lt;p&gt;In contrast to Deep Blue, the researchers developing AlphaGo didn’t need to have any prior knowledge of the rules of the game. The AI managed to learn the rules of the game and how to win all by itself. This was a feat of general intelligence, but there are some shortcomings to this kind of AI. What exactly has AlphaGo learned and was it worth the cost?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;AlphaGo belongs to a subset of Machine Learning known as Deep Learning (note that the naming of Deep Blue is a coincidence and it is not classified as Deep Learning) which is notorious for requiring a substantial amount of computational power to learn. Deep Learning AIs spend days churning data to learn how to solve a problem. A study on the energy consumption of hardware built for Deep Learning revealed that the learning process can &lt;a href=&quot;https://arxiv.org/abs/1906.02243&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;emit as much carbon&lt;/a&gt; as the lifetime of five midsize sedans. In addition to that, the intelligent behaviour learned is uninterpretable by even the developers themselves, i.e. we will never know why AlphaGo makes a particular move. Maybe this isn’t such a big problem in a game of Go as the results of the AI are phenomenal.&lt;/p&gt;
&lt;p&gt;The use of AI has amazingly made its way into governments and judicial systems, but has also been the centre of criticism from the media and public alike. In 2016, the Wisconsin Supreme Court ruled for judges to use it as an aid to assess the risk of recidivism. Little is understood about COMPAS since it uses trade secrets. This prompted two independent researchers to compare it to the most basic Machine Learning AI (a linear regression), and they found that there was no significant difference; COMPAS does nothing extraordinarily ‘intelligent’. To add insult to injury, an &lt;a href=&quot;https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;investigation&lt;/a&gt; into its risk assessments revealed that there is an inherent bias that favours white people to be classified as low-risk and black people to be classified as high-risk. This is a big concern in Machine Learning which is driven by historical data. If the data itself is inherently biased then the AI learning from it would be biased as well.&lt;/p&gt;
&lt;p&gt;The cherry on top for how AI is misunderstood was seen in Australia; the RoboDebt scheme. In 2016, the Australian government decided to enlist the help of AI to crack down on the abuse of their welfare system and clawback A$1.5 billion in three years. They implemented an automated debt recovery system (a seemingly Logic Programming AI) that was meant to accurately calculate how much a person owed or was owed by the government. Ironically, it has since incurred the Australian government A$1.2 billion in refunds and payouts.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;There were many things wrong with the implementation of the RoboDebt scheme, but let’s look at it from a purely technical standpoint. To this day, there hasn’t been any scrutiny against the AI used. Put simply it did exactly what it was meant to do and nothing more. The human checks and balances that were in place prior to the AI, however, were completely removed and they weren’t replaced. Fundamental errors in the system weren’t caught. In fact, the biggest technical scrutiny was from the way the Australian government averaged income. This was in place much before RoboDebt and its introduction only perpetuated the already existing problem at a rapid rate. Perhaps an AI is only as ‘intelligent’ as the people it is solving a problem for.&lt;/p&gt;
&lt;p&gt;Most of the AI stories that make it to the headlines greatly tip the scales of its perception and propagate misunderstanding in society. A single AI story shouldn’t shape it as a whole as there are many different types of AI, and we have only scratched the surface in this article. Most AI research doesn’t make it to the headlines. However, there are many great applications and use cases of AI which will shape our future, and only when it does will we hear about it. Until then all we can do is learn a little more.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Kheeran Naidu</name>
        
        
      </author>

      

      
        <category term="trending" />
      
        <category term="society" />
      

      
        <summary type="html">Artificial Intelligence (AI) is a term broadly used to describe some innovation that is trying to solve yet another problem. A survey by the UK government in 2019 on over 2,400 individuals reported that a good majority know what AI is, but only 12% know a lot about AI. This is no surprise, and the media doesn’t do well to clarify this uncertainty. An early definition of AI was proposed by Alan Turing 70 years ago. He described a method to determine if a machine exhibits intelligent behaviour, known as the 'Imitation Game' - a game where an interrogator judges if the responses to their questions are from a human or machine. Unsurprisingly, this is a great definition. However, as technology advances, problems that were once thought of as only solvable by humans have become ‘easily’ solvable by AI, and are no longer classified as AI problems. This paradox is known as the 'AI Effect'. Before breaking down some AI that you might have heard of, I want to be clear about something that is very often misunderstood. AI can be grouped into two categories: Logic Programming (rule-based intelligence) and Machine Learning (data-driven intelligence). In the former category, the intelligence of the AI is developed by researchers. They would very carefully study the domain of the problem and describe a set of rules and instructions which would best output a solution to the problem. In the latter category, intelligence is learned by AI. The researcher would typically use an existing model for learning (with tweakable settings) and apply it to historical data from the domain of the problem. Then the settings of the model which would output the best possible solution to the problem are found based on the data. The model with the optimal settings can then solve the problem. AI in both these categories has garnered a great deal of positive media attention, and rightfully so, but they have attributed to the great misunderstanding of AI that is widespread in society. Two examples of board game matches from the past 25 years apparently exhibit great strides in AI, but are mostly a testament to computing power rather than intelligent behaviour. A Logic Programming AI made history in 1997 when IBM’s supercomputer Deep Blue beat the then reigning world champion in a match of chess under standard tournament conditions. Deep Blue follows a set of logical rules to search for the best sequence of moves needed to win, with its prowess coming from its computational speed. At the time, it could evaluate about 200 million chess moves per second (7-12 turns ahead) and its strategy to win could be completely analysed and interpreted by the researchers, i.e. the reason why it made a particular move was fully understood. Although Deep Blue has no practical applications other than playing chess, it proved that AI could outperform human intelligence at a game often associated with requiring high intelligence. On a similar problem, a Machine Learning AI made history in 2016 when Google’s AlphaGo beat the 18-time world champion in a match of Go. Go is a far more complicated game than chess and it is arguably the most complex game ever devised. The number of possible moves from each position of a chess game is about 20 whereas a Go game is about 200. A Logic Programming AI would need to evaluate about 60 trillion moves to think just 6 turns ahead, so finding the best sequence of moves needed to win the game would be unfeasible. Google’s researchers used a model called Deep Reinforcement Learning to tackle the problem. This model of learning was developed in the 1980's and stemmed from research into animal psychology. In this case, the AI was initially presented with 100,000 games of Go and, given rewards when it won, it learned how to play the game. It then played against itself millions of times to iteratively get better, until it was better than a world champion Go player. Unfortunately, this intelligent behaviour comes at a cost; it took 3 days to train AlphaGo and it consumed what would usually be $2 million of Google's computational power. In contrast to Deep Blue, the researchers developing AlphaGo didn’t need to have any prior knowledge of the rules of the game. The AI managed to learn the rules of the game and how to win all by itself. This was a feat of general intelligence, but there are some shortcomings to this kind of AI. What exactly has AlphaGo learned and was it worth the cost?&amp;nbsp; AlphaGo belongs to a subset of Machine Learning known as Deep Learning (note that the naming of Deep Blue is a coincidence and it is not classified as Deep Learning) which is notorious for requiring a substantial amount of computational power to learn. Deep Learning AIs spend days churning data to learn how to solve a problem. A study on the energy consumption of hardware built for Deep Learning revealed that the learning process can emit as much carbon as the lifetime of five midsize sedans. In addition to that, the intelligent behaviour learned is uninterpretable by even the developers themselves, i.e. we will never know why AlphaGo makes a particular move. Maybe this isn’t such a big problem in a game of Go as the results of the AI are phenomenal. The use of AI has amazingly made its way into governments and judicial systems, but has also been the centre of criticism from the media and public alike. In 2016, the Wisconsin Supreme Court ruled for judges to use it as an aid to assess the risk of recidivism. Little is understood about COMPAS since it uses trade secrets. This prompted two independent researchers to compare it to the most basic Machine Learning AI (a linear regression), and they found that there was no significant difference; COMPAS does nothing extraordinarily ‘intelligent’. To add insult to injury, an investigation into its risk assessments revealed that there is an inherent bias that favours white people to be classified as low-risk and black people to be classified as high-risk. This is a big concern in Machine Learning which is driven by historical data. If the data itself is inherently biased then the AI learning from it would be biased as well. The cherry on top for how AI is misunderstood was seen in Australia; the RoboDebt scheme. In 2016, the Australian government decided to enlist the help of AI to crack down on the abuse of their welfare system and clawback A$1.5 billion in three years. They implemented an automated debt recovery system (a seemingly Logic Programming AI) that was meant to accurately calculate how much a person owed or was owed by the government. Ironically, it has since incurred the Australian government A$1.2 billion in refunds and payouts.&amp;nbsp; There were many things wrong with the implementation of the RoboDebt scheme, but let’s look at it from a purely technical standpoint. To this day, there hasn’t been any scrutiny against the AI used. Put simply it did exactly what it was meant to do and nothing more. The human checks and balances that were in place prior to the AI, however, were completely removed and they weren’t replaced. Fundamental errors in the system weren’t caught. In fact, the biggest technical scrutiny was from the way the Australian government averaged income. This was in place much before RoboDebt and its introduction only perpetuated the already existing problem at a rapid rate. Perhaps an AI is only as ‘intelligent’ as the people it is solving a problem for. Most of the AI stories that make it to the headlines greatly tip the scales of its perception and propagate misunderstanding in society. A single AI story shouldn’t shape it as a whole as there are many different types of AI, and we have only scratched the surface in this article. Most AI research doesn’t make it to the headlines. However, there are many great applications and use cases of AI which will shape our future, and only when it does will we hear about it. Until then all we can do is learn a little more.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Egyptian Mummies: A Story of the Colonial Abuse of History</title>
      <link href="/Egyptian-Mummies" rel="alternate" type="text/html" title="Egyptian Mummies: A Story of the Colonial Abuse of History" />
      <published>2021-01-29T00:00:00+00:00</published>
      <updated>2021-01-29T00:00:00+00:00</updated>
      <id>/Egyptian-Mummies</id>
      <content type="html" xml:base="/Egyptian-Mummies">&lt;p&gt;Mummies are probably among the most 'Egyptian' things to exist in our history books in spite of the fact that mummification is neither unique to ancient Egypt nor is it always man-made. The main reason behind the popularisation of mummification as a part of ancient Egyptian culture, apart from the academic fascination with the extraordinary methods employed in the process to mummify a dead body by ancient Egyptians, is the endless number of stories that exist about them. Very unsurprisingly, most of them are distorted and sometimes outright lies regarding the culture. Most, if not all of these, are rooted in absolute ignorance about history and sometimes, they exist due to a veiled effort to profit off of an exotic ancient African civilisation.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;As a believer in science and as a passionate reader of history, the process of mummification is rather interesting to me. The Egyptians were, however, not the first to mummify their dead. At least 2,000 years before them, the Chinchorro people of Southern American descent were already practising mummification. Of course, the methods employed vary from civilisation to civilisation. In addition, based on how well the bodies were preserved and other factors, it can be inferred that the procedure changed with time. Unfortunately, climate change has not been kind to mummies. The rising humidity levels have contributed to the spread of flesh-eating microbes, which are detrimental to the preservation of mummies. This makes studying them all the more difficult. Apart from this, the procedure employed for the poor was less extensive compared to that employed for the Pharaohs. Unfortunately, there are no proper accounts of how the mummification was done in ancient Egypt left behind by the Egyptians.&lt;/p&gt;
&lt;p&gt;The only two ancient texts that lay out the mummification techniques are the accounts of Diodorus and Herodotus from when they travelled to Egypt. The famous &lt;em&gt;Book of the Dead&lt;/em&gt;, which has made appearances in various cult classics, does not extensively talk about the methods employed. Instead, it provides various spells and rituals that could potentially help the dead enter the afterlife. In 1994, two scientists used the ancient Egyptian mummification methods on a victim of heart attack successfully, in order to get a greater depth of understanding of the procedure.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;The process of mummification is very elaborate. It began with removing the internal organs, with the exception of the heart. The ancient Egyptian religion propagated a belief that the dead were judged by their heart on entering the underworld, before proceeding on to the afterlife. They would thus use an amulet, commonly referred to as the heart scarab, to protect it. The overall mummification process took about 40 days. The Egyptians did not limit this practice to humans - they mummified various animals like cats, ibises, hawks, crocodiles, rats and lizards, primarily for religious reasons.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;The practice of mummification in Egypt died out between the 4th and 7th century AD, which is around the time when many of the Egyptians converted to Christianity. It has been estimated that over a period of about 3,000 years over 70 million mummies were made in Egypt. The Egyptian civilisation existed for much longer than this. It is believed that anthropogenic mummification was introduced into practice around 2,600 BC, that is, during the Fourth or Fifth Dynasties. There are older mummies available but these have been preserved naturally, especially owing to the fact that Egypt has zero measurable rainfall.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Egyptian mummies have always fascinated people. While today we are aware of the psyche behind the process and the procedure involved in mummifying the remains of humans, the same cannot be said about those who came much before us and after the decline of ancient Egypt. Scholars of the 18th century were interested in knowing what lay under the wrappings of a mummy. Soon 'Mummy Unwrapping Parties' were popularised by those who could afford them, in their private homes. Later, such unwrapping ceremonies would also be organised in public theatres. Many of these people, who conducted these events, had no knowledge of medical sciences. It was simply a source of entertainment, a way to feed the public’s fascination with mummies and of course, as a symbol of wealth.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;'Mummy dust' makes appearances throughout history in the most absurd places and manners. King Charles II believed that mummy dust could potentially contribute to his greatness and so, he was known to rub it onto his skin. King Francis I of France had a similar belief - he thought that it would make him stronger and so he took a pinch of mummy every day with rhubarb. An abstract published in the &lt;em&gt;Proceedings of the Royal Society of Medicine&lt;/em&gt; journal of 1927 puts forward the idea that between the 12th and the 17th centuries, medicines were prepared from powdered mummies. In fact, 'mummy medicine' was actually very popular around this time and an unrecorded number of mummies were disentombed and burned to prepare the medication. People at the time believed that bitumen had medicinal properties and it was thought that mummification involved embalming the body with bitumen. However, this was actually very uncommon - most mummies were embalmed with resins.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;The most commonly encountered occurrence of the Egyptian mummy in popular culture is the undead mummy. Going by Egyptian mythology, this is a rather weird concept. Their religious beliefs encouraged the idea that living beings move on to the after-life after death. A walking mummy in that context is simply absurd. How then was this trope born?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;The mummy genre has its origins in the 19th century. This was when Egypt was first colonised by France and then by Victorian Britain. The colonial romanticisation of the East resulted in most of these early stories, which presented mummies as mostly female and more often than not, the love interest of the protagonist. This is one of the numerous examples of sexualised orientalism born during the age of colonialism. Some of the greatest writers of the age delved into this new genre: &lt;em&gt;The Jewel of the Seven Stars&lt;/em&gt; by Bram Stoker and Sir Arthur Canon Doyle’s &lt;em&gt;The Ring of Thoth&lt;/em&gt; are two such examples. In H D Everett’s &lt;em&gt;Iras: A Mystery&lt;/em&gt;, the protagonist ends up marrying a mummy, which turns into a beautiful woman. In 1845, Edgar Allan Poe wrote &lt;em&gt;Some Words with a Mummy&lt;/em&gt;. This piece was a satire unlike most of the stories about mummies written at the time.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;The 1892 novel &lt;em&gt;Lot No. 249&lt;/em&gt;, written by Sir. Doyle is believed to be the first to portray a mummy as a dangerous creature. It was only in the 1930's that the 'monster mummy' started making frequent appearances. The 'romantic mummy' would make a comeback only in the late 20th century with Anne Rice’s 1989 novel, &lt;em&gt;The Mummy, or Ramses the Damned&lt;/em&gt;. This novel, unlike most of the 19th century ones, involved a sexual relationship between a male mummy (instead of a female) and a female archaeologist.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Romanticised and dangerous undead mummies would increasingly dominate books and screens from this point on, in the form of novellas, TV series, video games and so on and so forth. The &lt;em&gt;Goosebumps&lt;/em&gt; series by R L Stine has quite a few of them as monsters. Marvel Comics has its own mummies like N’Kantu. In the very popular &lt;em&gt;Ben 10&lt;/em&gt; franchise, Thep Khufans is a race of alien mummies. The lead character Ben Tennyson has ten alien forms, one of which is Snare-oh, previously named Benmummy, which is a Thep Khufan. In video games, mummies make notable appearances as hostile creatures thriving in deserts in &lt;em&gt;Terraria &lt;/em&gt;and as an immortal creature named Kan-Ra in&lt;em&gt; Killer Instinct&lt;/em&gt;, among others. In several editions of &lt;em&gt;Dungeons and Dragons&lt;/em&gt;, mummies have found a place in the form of Bog Mummies, Hunefers, Mummy Lords, etc. Various films also have Egyptian mummies as characters like Ahkmenrah from the &lt;em&gt;Night at the Museum &lt;/em&gt;and Murray the Mummy from &lt;em&gt;Hotel Transylvania.&lt;/em&gt; Almost nothing in any of these depictions has any similarity with the ancient Egyptian ideas or understanding of mummies and they are simply a result of colonial fascination with oriental culture.&lt;/p&gt;
&lt;p&gt;While walking undead mummies are a creation of the West and have no Egyptian origins, modern science has been trying to make this a reality, at least in a way. DNA capable of being analysed was found in mummies dating back to 2,012 BC by scientists who wish to clone mummies. A group of researchers recreated the voice of a 3,000-year-old mummy of an ancient Egyptian priest, Nesyamun using 3-D printing and body-scanning technology, effectively bringing him to life in 2020. Of course, none of these actually have the effect that the various stories perpetuate - a monster or a lover.&lt;/p&gt;
&lt;p&gt;A common plot in many Egyptian mummy-centric stories of today involves the 'Mummy’s Curse' or the 'Pharaoh’s Curse'. Now the question is, did ancient Egyptians believe in such a curse? The people of ancient Egypt were very particular about protecting tombs and mummies. Many of the pharaohs were entombed in a spot in the famous Valley of the Kings. According to the ancient Egyptian myths, Meretseger, a goddess who took the form of a cobra, protects the Valley of the Kings by blinding or poisoning tomb robbers. There were various punishments put in place to prevent tomb robbery in ancient times. Tomb raiders, who were caught, had the soles of their feet beaten before being publicly impaled on a sharp wooden stick.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Various deaths have been attributed to the Pharaoh’s Curse in the more recent past. The most famous among them is possibly the so-called Curse of Tutankhamun, which is popularly believed to have claimed over six lives in the years following the discovery of the tomb of the boy king in 1922. One of the victims is Lord Carnarvon, the sponsor of the expedition, who died six weeks after the historic event due to supposed blood poisoning. However, Howard Carter, the seemingly prime target of the supposed curses because of the fact that he was responsible for the discovery, died only in 1939, almost 17 years after the opening of the tomb. Only six of the 23 people present during the opening died in a decade following the discovery. With statistics like these, many refuse to believe that a curse was involved in the deaths.&lt;/p&gt;
&lt;p&gt;Many people have tried to explain these deaths. It is commonly believed to be biological in nature. Egyptian tombs have various items intended to help on the dead’s journey to the afterlife, alongside the sarcophagus. This includes food. It is possible that pathogens thrive in such sealed tombs. Investigations into this matter revealed that some tombs have mould-like &lt;em&gt;Aspergillus niger&lt;/em&gt; and &lt;em&gt;Aspergillus flavus&lt;/em&gt;, and bacteria like &lt;em&gt;Pseudomonas&lt;/em&gt; and &lt;em&gt;Staphylococcus&lt;/em&gt;. These are capable of causing various lung ailments. However, most scientists agree that these are not very dangerous. F DeWolfe Miller, Emeritus Professor of Epidemiology at the University of Hawaii at Manoa, agrees with this. He has been quoted as saying that he knows of no archaeologist or tourist who has suffered the adverse effects of tomb toxins.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Howard Carter believed that taking the conditions of Upper Egypt of the 1920's into consideration, Lord Carnarvon was probably safer inside the tomb, a statement that many scientists agree with. In this context, Miller was quoted saying, “The idea that an underground tomb, after 3,000 years, would have some kind of bizarre microorganism in it that's going to kill somebody six weeks later and make it look exactly like [blood poisoning] is very hard to believe”. The cause of the deaths is thus still up for debate.&lt;/p&gt;
&lt;p&gt;The belief in the curses as being the cause has been around for a while now but did they exist back during the ancient times? Late egyptologist Dominic Montserrat was quoted by &lt;em&gt;The Independent&lt;/em&gt; at the conclusion of extensive research into the matter as follows: “My research has not only confirmed that there is, of course, no ancient Egyptian origin of the mummy's curse concept, but, more importantly, it also reveals that it didn't originate in the 1923 press publicity about the discovery of Tutankhamen's tomb either”. The mummy’s curse concept is also not a contribution of Hollywood. In fact, it dates back to the 1800's. One of the earliest examples of this is Louisa May Alcott’s &lt;em&gt;Lost in a Pyramid &lt;/em&gt;or &lt;em&gt;The Mummy’s Curse&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Not all egyptologists agree with this though. Salima Ikram, who is an Egyptologist at the American University of Cairo believes that a form of curses did in fact exist in ancient Egyptian times as a primitive security system against the desecration of the tomb or any form of tomb robbery. According to Ikram, some of the mastaba (early Egyptian non-pyramidal tomb) walls located at Saqqara and Giza have curses inscribed on them. She has been quoted saying, “They tend to threaten desecrators with divine retribution by the council of the gods or a death by crocodiles, or lions, or scorpions, or snakes”.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Due to the expansive availability of various stories regarding the ancient Egyptians, it is difficult to know what among these is actually historically accurate. These stories have had the impact of this ancient civilisation is widely viewed as a land of extremely religious people who practised magic, making it difficult for many, including Elon Musk, to believe that these people built the magnificent pyramids, among other similar astonishing accomplishments. Of course, this is not the first time that Westerners have questioned the capabilities of the people of ancient Asian, South American or African civilisations. Quite frankly, much of what we know today through stories are solely a result of the colonial world’s unhealthy fascination with the exotic cultures and are not rooted in real history. The idea of undead mummies causing devastation or the fact that there are people who are actively seeking to hunt down specific mummies and disentomb them (archaeologists), would probably scandalise the ancient Egyptians. At the end of the day, 'disturbing' the dead in any culture is considered disrespectful. However, people have made exceptions for the ancient Egyptians in the name of gaining knowledge, preparing medicines and sometimes, for the riches enclosed within the walls of the tombs, for a very long time now.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Oindrila Ghosh</name>
        
        
      </author>

      

      
        <category term="society" />
      

      
        <summary type="html">Mummies are probably among the most 'Egyptian' things to exist in our history books in spite of the fact that mummification is neither unique to ancient Egypt nor is it always man-made. The main reason behind the popularisation of mummification as a part of ancient Egyptian culture, apart from the academic fascination with the extraordinary methods employed in the process to mummify a dead body by ancient Egyptians, is the endless number of stories that exist about them. Very unsurprisingly, most of them are distorted and sometimes outright lies regarding the culture. Most, if not all of these, are rooted in absolute ignorance about history and sometimes, they exist due to a veiled effort to profit off of an exotic ancient African civilisation.&amp;nbsp; As a believer in science and as a passionate reader of history, the process of mummification is rather interesting to me. The Egyptians were, however, not the first to mummify their dead. At least 2,000 years before them, the Chinchorro people of Southern American descent were already practising mummification. Of course, the methods employed vary from civilisation to civilisation. In addition, based on how well the bodies were preserved and other factors, it can be inferred that the procedure changed with time. Unfortunately, climate change has not been kind to mummies. The rising humidity levels have contributed to the spread of flesh-eating microbes, which are detrimental to the preservation of mummies. This makes studying them all the more difficult. Apart from this, the procedure employed for the poor was less extensive compared to that employed for the Pharaohs. Unfortunately, there are no proper accounts of how the mummification was done in ancient Egypt left behind by the Egyptians. The only two ancient texts that lay out the mummification techniques are the accounts of Diodorus and Herodotus from when they travelled to Egypt. The famous Book of the Dead, which has made appearances in various cult classics, does not extensively talk about the methods employed. Instead, it provides various spells and rituals that could potentially help the dead enter the afterlife. In 1994, two scientists used the ancient Egyptian mummification methods on a victim of heart attack successfully, in order to get a greater depth of understanding of the procedure.&amp;nbsp; The process of mummification is very elaborate. It began with removing the internal organs, with the exception of the heart. The ancient Egyptian religion propagated a belief that the dead were judged by their heart on entering the underworld, before proceeding on to the afterlife. They would thus use an amulet, commonly referred to as the heart scarab, to protect it. The overall mummification process took about 40 days. The Egyptians did not limit this practice to humans - they mummified various animals like cats, ibises, hawks, crocodiles, rats and lizards, primarily for religious reasons.&amp;nbsp; The practice of mummification in Egypt died out between the 4th and 7th century AD, which is around the time when many of the Egyptians converted to Christianity. It has been estimated that over a period of about 3,000 years over 70 million mummies were made in Egypt. The Egyptian civilisation existed for much longer than this. It is believed that anthropogenic mummification was introduced into practice around 2,600 BC, that is, during the Fourth or Fifth Dynasties. There are older mummies available but these have been preserved naturally, especially owing to the fact that Egypt has zero measurable rainfall.&amp;nbsp; Egyptian mummies have always fascinated people. While today we are aware of the psyche behind the process and the procedure involved in mummifying the remains of humans, the same cannot be said about those who came much before us and after the decline of ancient Egypt. Scholars of the 18th century were interested in knowing what lay under the wrappings of a mummy. Soon 'Mummy Unwrapping Parties' were popularised by those who could afford them, in their private homes. Later, such unwrapping ceremonies would also be organised in public theatres. Many of these people, who conducted these events, had no knowledge of medical sciences. It was simply a source of entertainment, a way to feed the public’s fascination with mummies and of course, as a symbol of wealth.&amp;nbsp; 'Mummy dust' makes appearances throughout history in the most absurd places and manners. King Charles II believed that mummy dust could potentially contribute to his greatness and so, he was known to rub it onto his skin. King Francis I of France had a similar belief - he thought that it would make him stronger and so he took a pinch of mummy every day with rhubarb. An abstract published in the Proceedings of the Royal Society of Medicine journal of 1927 puts forward the idea that between the 12th and the 17th centuries, medicines were prepared from powdered mummies. In fact, 'mummy medicine' was actually very popular around this time and an unrecorded number of mummies were disentombed and burned to prepare the medication. People at the time believed that bitumen had medicinal properties and it was thought that mummification involved embalming the body with bitumen. However, this was actually very uncommon - most mummies were embalmed with resins.&amp;nbsp; The most commonly encountered occurrence of the Egyptian mummy in popular culture is the undead mummy. Going by Egyptian mythology, this is a rather weird concept. Their religious beliefs encouraged the idea that living beings move on to the after-life after death. A walking mummy in that context is simply absurd. How then was this trope born?&amp;nbsp; The mummy genre has its origins in the 19th century. This was when Egypt was first colonised by France and then by Victorian Britain. The colonial romanticisation of the East resulted in most of these early stories, which presented mummies as mostly female and more often than not, the love interest of the protagonist. This is one of the numerous examples of sexualised orientalism born during the age of colonialism. Some of the greatest writers of the age delved into this new genre: The Jewel of the Seven Stars by Bram Stoker and Sir Arthur Canon Doyle’s The Ring of Thoth are two such examples. In H D Everett’s Iras: A Mystery, the protagonist ends up marrying a mummy, which turns into a beautiful woman. In 1845, Edgar Allan Poe wrote Some Words with a Mummy. This piece was a satire unlike most of the stories about mummies written at the time.&amp;nbsp; The 1892 novel Lot No. 249, written by Sir. Doyle is believed to be the first to portray a mummy as a dangerous creature. It was only in the 1930's that the 'monster mummy' started making frequent appearances. The 'romantic mummy' would make a comeback only in the late 20th century with Anne Rice’s 1989 novel, The Mummy, or Ramses the Damned. This novel, unlike most of the 19th century ones, involved a sexual relationship between a male mummy (instead of a female) and a female archaeologist.&amp;nbsp; Romanticised and dangerous undead mummies would increasingly dominate books and screens from this point on, in the form of novellas, TV series, video games and so on and so forth. The Goosebumps series by R L Stine has quite a few of them as monsters. Marvel Comics has its own mummies like N’Kantu. In the very popular Ben 10 franchise, Thep Khufans is a race of alien mummies. The lead character Ben Tennyson has ten alien forms, one of which is Snare-oh, previously named Benmummy, which is a Thep Khufan. In video games, mummies make notable appearances as hostile creatures thriving in deserts in Terraria and as an immortal creature named Kan-Ra in Killer Instinct, among others. In several editions of Dungeons and Dragons, mummies have found a place in the form of Bog Mummies, Hunefers, Mummy Lords, etc. Various films also have Egyptian mummies as characters like Ahkmenrah from the Night at the Museum and Murray the Mummy from Hotel Transylvania. Almost nothing in any of these depictions has any similarity with the ancient Egyptian ideas or understanding of mummies and they are simply a result of colonial fascination with oriental culture. While walking undead mummies are a creation of the West and have no Egyptian origins, modern science has been trying to make this a reality, at least in a way. DNA capable of being analysed was found in mummies dating back to 2,012 BC by scientists who wish to clone mummies. A group of researchers recreated the voice of a 3,000-year-old mummy of an ancient Egyptian priest, Nesyamun using 3-D printing and body-scanning technology, effectively bringing him to life in 2020. Of course, none of these actually have the effect that the various stories perpetuate - a monster or a lover. A common plot in many Egyptian mummy-centric stories of today involves the 'Mummy’s Curse' or the 'Pharaoh’s Curse'. Now the question is, did ancient Egyptians believe in such a curse? The people of ancient Egypt were very particular about protecting tombs and mummies. Many of the pharaohs were entombed in a spot in the famous Valley of the Kings. According to the ancient Egyptian myths, Meretseger, a goddess who took the form of a cobra, protects the Valley of the Kings by blinding or poisoning tomb robbers. There were various punishments put in place to prevent tomb robbery in ancient times. Tomb raiders, who were caught, had the soles of their feet beaten before being publicly impaled on a sharp wooden stick.&amp;nbsp; Various deaths have been attributed to the Pharaoh’s Curse in the more recent past. The most famous among them is possibly the so-called Curse of Tutankhamun, which is popularly believed to have claimed over six lives in the years following the discovery of the tomb of the boy king in 1922. One of the victims is Lord Carnarvon, the sponsor of the expedition, who died six weeks after the historic event due to supposed blood poisoning. However, Howard Carter, the seemingly prime target of the supposed curses because of the fact that he was responsible for the discovery, died only in 1939, almost 17 years after the opening of the tomb. Only six of the 23 people present during the opening died in a decade following the discovery. With statistics like these, many refuse to believe that a curse was involved in the deaths. Many people have tried to explain these deaths. It is commonly believed to be biological in nature. Egyptian tombs have various items intended to help on the dead’s journey to the afterlife, alongside the sarcophagus. This includes food. It is possible that pathogens thrive in such sealed tombs. Investigations into this matter revealed that some tombs have mould-like Aspergillus niger and Aspergillus flavus, and bacteria like Pseudomonas and Staphylococcus. These are capable of causing various lung ailments. However, most scientists agree that these are not very dangerous. F DeWolfe Miller, Emeritus Professor of Epidemiology at the University of Hawaii at Manoa, agrees with this. He has been quoted as saying that he knows of no archaeologist or tourist who has suffered the adverse effects of tomb toxins.&amp;nbsp; Howard Carter believed that taking the conditions of Upper Egypt of the 1920's into consideration, Lord Carnarvon was probably safer inside the tomb, a statement that many scientists agree with. In this context, Miller was quoted saying, “The idea that an underground tomb, after 3,000 years, would have some kind of bizarre microorganism in it that's going to kill somebody six weeks later and make it look exactly like [blood poisoning] is very hard to believe”. The cause of the deaths is thus still up for debate. The belief in the curses as being the cause has been around for a while now but did they exist back during the ancient times? Late egyptologist Dominic Montserrat was quoted by The Independent at the conclusion of extensive research into the matter as follows: “My research has not only confirmed that there is, of course, no ancient Egyptian origin of the mummy's curse concept, but, more importantly, it also reveals that it didn't originate in the 1923 press publicity about the discovery of Tutankhamen's tomb either”. The mummy’s curse concept is also not a contribution of Hollywood. In fact, it dates back to the 1800's. One of the earliest examples of this is Louisa May Alcott’s Lost in a Pyramid or The Mummy’s Curse. Not all egyptologists agree with this though. Salima Ikram, who is an Egyptologist at the American University of Cairo believes that a form of curses did in fact exist in ancient Egyptian times as a primitive security system against the desecration of the tomb or any form of tomb robbery. According to Ikram, some of the mastaba (early Egyptian non-pyramidal tomb) walls located at Saqqara and Giza have curses inscribed on them. She has been quoted saying, “They tend to threaten desecrators with divine retribution by the council of the gods or a death by crocodiles, or lions, or scorpions, or snakes”.&amp;nbsp; Due to the expansive availability of various stories regarding the ancient Egyptians, it is difficult to know what among these is actually historically accurate. These stories have had the impact of this ancient civilisation is widely viewed as a land of extremely religious people who practised magic, making it difficult for many, including Elon Musk, to believe that these people built the magnificent pyramids, among other similar astonishing accomplishments. Of course, this is not the first time that Westerners have questioned the capabilities of the people of ancient Asian, South American or African civilisations. Quite frankly, much of what we know today through stories are solely a result of the colonial world’s unhealthy fascination with the exotic cultures and are not rooted in real history. The idea of undead mummies causing devastation or the fact that there are people who are actively seeking to hunt down specific mummies and disentomb them (archaeologists), would probably scandalise the ancient Egyptians. At the end of the day, 'disturbing' the dead in any culture is considered disrespectful. However, people have made exceptions for the ancient Egyptians in the name of gaining knowledge, preparing medicines and sometimes, for the riches enclosed within the walls of the tombs, for a very long time now.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">The One Time You Shouldn’t Be Appropriate</title>
      <link href="/The-One-Time-You-Shouldn-t-Be-Appropriate" rel="alternate" type="text/html" title="The One Time You Shouldn't Be Appropriate" />
      <published>2021-01-11T00:00:00+00:00</published>
      <updated>2021-01-11T00:00:00+00:00</updated>
      <id>/The-One-Time-You-Shouldn-t-Be-Appropriate</id>
      <content type="html" xml:base="/The-One-Time-You-Shouldn-t-Be-Appropriate">&lt;p&gt;Jack Skellington, uninspired, and in a rut, discovers Christmas Town, falls head over heels with a culture that is not his to begin with, does not talk to a single person who actually lives there, and thus fails to understand the significance of its elements, then goes on to falsify their image in order to appeal to a different audience, and despite their reluctance, decides that he, the Pumpkin King of Halloweentown, can in fact do Christmas better than Christmas Town. At which he fails. Spectacularly.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Unlike what John Mulaney would say, we, dear readers, absolutely do have the time to unpack all of that.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Nightmare Before Christmas&lt;/em&gt; (1993) is an excellent conversation starter and explanation tool if you are completely new to the concept of cultural appropriation. The &lt;em&gt;Cambridge Dictionary&lt;/em&gt; defines cultural appropriation as “&lt;em&gt;the&amp;nbsp;&lt;/em&gt;&lt;a href=&quot;https://dictionary.cambridge.org/dictionary/english/act&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;&lt;em&gt;act&lt;/em&gt;&lt;/a&gt;&lt;em&gt;&amp;nbsp;of taking or using things from a&amp;nbsp;&lt;/em&gt;&lt;a href=&quot;https://dictionary.cambridge.org/dictionary/english/culture&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;&lt;em&gt;culture&lt;/em&gt;&lt;/a&gt;&lt;em&gt;&amp;nbsp;that is not&amp;nbsp;&lt;/em&gt;&lt;a href=&quot;https://dictionary.cambridge.org/dictionary/english/your&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;&lt;em&gt;your&lt;/em&gt;&lt;/a&gt;&lt;em&gt;&amp;nbsp;own,&amp;nbsp;&lt;/em&gt;&lt;a href=&quot;https://dictionary.cambridge.org/dictionary/english/especially&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;&lt;em&gt;especially&lt;/em&gt;&lt;/a&gt;&lt;em&gt;&amp;nbsp;without&amp;nbsp;&lt;/em&gt;&lt;a href=&quot;https://dictionary.cambridge.org/dictionary/english/showing&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;&lt;em&gt;showing&lt;/em&gt;&lt;/a&gt;&lt;em&gt;&amp;nbsp;that you&amp;nbsp;&lt;/em&gt;&lt;a href=&quot;https://dictionary.cambridge.org/dictionary/english/understand&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;&lt;em&gt;understand&lt;/em&gt;&lt;/a&gt;&lt;em&gt;&amp;nbsp;or&amp;nbsp;&lt;/em&gt;&lt;a href=&quot;https://dictionary.cambridge.org/dictionary/english/respect&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;&lt;em&gt;respect&lt;/em&gt;&lt;/a&gt;&lt;em&gt;&amp;nbsp;the original&amp;nbsp;&lt;/em&gt;&lt;a href=&quot;https://dictionary.cambridge.org/dictionary/english/culture&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;&lt;em&gt;culture&lt;/em&gt;&lt;/a&gt;&lt;em&gt;.&lt;/em&gt;”&lt;/p&gt;
&lt;p&gt;As the story begins, Jack is shown disheartened with the usual Halloween festivities – as he leads the celebration each year, he privately grows weary of it. Thus, it is an extremely delightful experience for him when he discovers the even gates representing different festivals – and goes through the one leading to the Christmas Town. He discovers a brand new world, a different festival, and a wildly different way of living. Enthused, he takes back some of the things he likes (Christmas baubles, candy canes, etc.) in an attempt to understand what it was that made Christmas so magical. Notice how he doesn’t even try to talk to the residents of the town, who would have answered his questions in depth had he just thought to ask. The residents of Halloween Town are also confused, since without any context as to why things are done a certain way, all the Christmas elements can only be compared to what they already know, and do not come at par. As expected, Jack fails at understanding Christmas because even after the multiple experiments and equations back at his crib, he is, after all, using his egregiously narrow perspective to explain something totally out of his scope.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;When Jack can neither understand nor rationally explain Christmas to anyone – not even to himself, he simply decides that he is going to ‘improve Christmas’ as it is unfair that only Christmas Town gets to celebrate it. He tasks different citizens with jobs like ‘singing carols’, ‘preparing gifts’ and even ‘kidnapping Santa’ to make sure he is fully prepared to lead the festivities. Despite getting warned not to proceed, Jack goes on to botch the entire festival, ruin Christmas for the normal world, and as a return gift, gets shot out of the sky.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;A key fact about cultural appropriation that is missing from the above story is that it is always people of the dominant culture that appropriate the culture of the minorities. While Christmas is in no way a ‘minority culture’ – the &lt;em&gt;Nightmare before Christmas&lt;/em&gt; serves some excellent points in what not to do when taking inspiration from another culture.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Why is cultural appropriation a problem? What is so wrong in getting influenced by another culture, especially in a world where there is increased access to everything that exists on earth? The problem arises when the ‘influence’ and ‘inspiration’ is taken for the wrong reasons. When white musicians use music pioneered by black artists and make a killing off of it, when fashion houses host shows inspired by black culture but have only white people modelling for them, or when white people dress up as geishas or put tribal paint on for Halloween – that is when they cross the line from appreciation to appropriation. It is problematic when artists like Zendaya get ridiculed for wearing dreadlocks (which is a common hairstyle for people of African ethnicities) while white celebrities like many of the Kardashian – Jenners continue to do the same and garner millions of Instagram likes and shares despite significant backlash from the black community.&lt;/p&gt;
&lt;p&gt;As an Indian, I am not new to watching my culture get appropriated for profit or aesthetic, even though every new instance in an apparently increasingly educated world does make me infuriated. Music festivals are a breeding ground for not just STDs, but also for blatant disrespect of ‘exotic’ cultures, as white people wear bindis, apply henna, or get dressed in the traditional garb of other cultures, all for the aesthetic. While doing the same in normal settings would lead to immense amounts of ridicule not to the white person, but to the Indian just trying to wear something common in their community. White people may profit hugely and be seen as ‘cool’ from doing the same things that people of colour have been doing since long. Even many K-pop artists have remorselessly appropriated different cultures – from randomly wearing dreadlocks in rap videos, to the girl group Blackpink most recently using a statue of the Hindu god Ganesha in an unrelated music video, no amount of fan led educational discourse seems to stop them in their tracks.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Cultural appropriation does more than harm the sentiments of the non-dominant community. It perpetuates offensive stereotypes, and may blind people to the true origins of the culture being appropriated. When Miley Cyrus famously twerked in her ‘We Can’t Stop’ music video – generations of people formed the misconception that it was she who had invented the dance move rather than African-American artists from New Orleans in the early 1990's. This kind of appropriation also opens the gate to fetishisation of concepts not meant to be sexual – the glaring popularity of ‘sexy kimono’ or ‘sexy geisha outfit’ searches during Halloween in America is a testament to this very fact. Kimono is the traditional dress of the Japanese woman, with a rich history that does not deserve to be reduced to a costume for an entitled brat. And geishas have a long history of being dignified artists and performers, not some sort of glorified sex workers. In fact, the entire premise of Halloween is based on appropriation– the people who most fervently celebrate it remain ignorant of its roots, using it as an excuse to wear costumes that degrade cultures different from them. The ancient Samhain tradition of wearing masks to scare away evil spirits has devolved into misogynistic and appropriative debauchery for the amusement of many.&lt;/p&gt;
&lt;p&gt;Using elements of a different culture merely ‘for the aesthetic’ and because you apparently can’t think of something original – cultural appropriation is, in a nutshell, plagiarism of the oppressed. And in a world where Google can be opened on nearly any electronic device, there is no excuse for anyone to remain ignorant of the culture they are supposedly being inspired by.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Swati Mathuria</name>
        
        
      </author>

      

      
        <category term="society" />
      

      
        <summary type="html">Jack Skellington, uninspired, and in a rut, discovers Christmas Town, falls head over heels with a culture that is not his to begin with, does not talk to a single person who actually lives there, and thus fails to understand the significance of its elements, then goes on to falsify their image in order to appeal to a different audience, and despite their reluctance, decides that he, the Pumpkin King of Halloweentown, can in fact do Christmas better than Christmas Town. At which he fails. Spectacularly.&amp;nbsp;&amp;nbsp; Unlike what John Mulaney would say, we, dear readers, absolutely do have the time to unpack all of that.&amp;nbsp; The Nightmare Before Christmas (1993) is an excellent conversation starter and explanation tool if you are completely new to the concept of cultural appropriation. The Cambridge Dictionary defines cultural appropriation as “the&amp;nbsp;act&amp;nbsp;of taking or using things from a&amp;nbsp;culture&amp;nbsp;that is not&amp;nbsp;your&amp;nbsp;own,&amp;nbsp;especially&amp;nbsp;without&amp;nbsp;showing&amp;nbsp;that you&amp;nbsp;understand&amp;nbsp;or&amp;nbsp;respect&amp;nbsp;the original&amp;nbsp;culture.” As the story begins, Jack is shown disheartened with the usual Halloween festivities – as he leads the celebration each year, he privately grows weary of it. Thus, it is an extremely delightful experience for him when he discovers the even gates representing different festivals – and goes through the one leading to the Christmas Town. He discovers a brand new world, a different festival, and a wildly different way of living. Enthused, he takes back some of the things he likes (Christmas baubles, candy canes, etc.) in an attempt to understand what it was that made Christmas so magical. Notice how he doesn’t even try to talk to the residents of the town, who would have answered his questions in depth had he just thought to ask. The residents of Halloween Town are also confused, since without any context as to why things are done a certain way, all the Christmas elements can only be compared to what they already know, and do not come at par. As expected, Jack fails at understanding Christmas because even after the multiple experiments and equations back at his crib, he is, after all, using his egregiously narrow perspective to explain something totally out of his scope.&amp;nbsp; When Jack can neither understand nor rationally explain Christmas to anyone – not even to himself, he simply decides that he is going to ‘improve Christmas’ as it is unfair that only Christmas Town gets to celebrate it. He tasks different citizens with jobs like ‘singing carols’, ‘preparing gifts’ and even ‘kidnapping Santa’ to make sure he is fully prepared to lead the festivities. Despite getting warned not to proceed, Jack goes on to botch the entire festival, ruin Christmas for the normal world, and as a return gift, gets shot out of the sky.&amp;nbsp; A key fact about cultural appropriation that is missing from the above story is that it is always people of the dominant culture that appropriate the culture of the minorities. While Christmas is in no way a ‘minority culture’ – the Nightmare before Christmas serves some excellent points in what not to do when taking inspiration from another culture.&amp;nbsp; Why is cultural appropriation a problem? What is so wrong in getting influenced by another culture, especially in a world where there is increased access to everything that exists on earth? The problem arises when the ‘influence’ and ‘inspiration’ is taken for the wrong reasons. When white musicians use music pioneered by black artists and make a killing off of it, when fashion houses host shows inspired by black culture but have only white people modelling for them, or when white people dress up as geishas or put tribal paint on for Halloween – that is when they cross the line from appreciation to appropriation. It is problematic when artists like Zendaya get ridiculed for wearing dreadlocks (which is a common hairstyle for people of African ethnicities) while white celebrities like many of the Kardashian – Jenners continue to do the same and garner millions of Instagram likes and shares despite significant backlash from the black community. As an Indian, I am not new to watching my culture get appropriated for profit or aesthetic, even though every new instance in an apparently increasingly educated world does make me infuriated. Music festivals are a breeding ground for not just STDs, but also for blatant disrespect of ‘exotic’ cultures, as white people wear bindis, apply henna, or get dressed in the traditional garb of other cultures, all for the aesthetic. While doing the same in normal settings would lead to immense amounts of ridicule not to the white person, but to the Indian just trying to wear something common in their community. White people may profit hugely and be seen as ‘cool’ from doing the same things that people of colour have been doing since long. Even many K-pop artists have remorselessly appropriated different cultures – from randomly wearing dreadlocks in rap videos, to the girl group Blackpink most recently using a statue of the Hindu god Ganesha in an unrelated music video, no amount of fan led educational discourse seems to stop them in their tracks.&amp;nbsp; Cultural appropriation does more than harm the sentiments of the non-dominant community. It perpetuates offensive stereotypes, and may blind people to the true origins of the culture being appropriated. When Miley Cyrus famously twerked in her ‘We Can’t Stop’ music video – generations of people formed the misconception that it was she who had invented the dance move rather than African-American artists from New Orleans in the early 1990's. This kind of appropriation also opens the gate to fetishisation of concepts not meant to be sexual – the glaring popularity of ‘sexy kimono’ or ‘sexy geisha outfit’ searches during Halloween in America is a testament to this very fact. Kimono is the traditional dress of the Japanese woman, with a rich history that does not deserve to be reduced to a costume for an entitled brat. And geishas have a long history of being dignified artists and performers, not some sort of glorified sex workers. In fact, the entire premise of Halloween is based on appropriation– the people who most fervently celebrate it remain ignorant of its roots, using it as an excuse to wear costumes that degrade cultures different from them. The ancient Samhain tradition of wearing masks to scare away evil spirits has devolved into misogynistic and appropriative debauchery for the amusement of many. Using elements of a different culture merely ‘for the aesthetic’ and because you apparently can’t think of something original – cultural appropriation is, in a nutshell, plagiarism of the oppressed. And in a world where Google can be opened on nearly any electronic device, there is no excuse for anyone to remain ignorant of the culture they are supposedly being inspired by.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Virtual Classrooms and Open Book Examinations</title>
      <link href="/Virtual-Classrooms-and-Open-Book-Examinations" rel="alternate" type="text/html" title="Virtual Classrooms and Open Book Examinations" />
      <published>2020-12-12T00:00:00+00:00</published>
      <updated>2020-12-12T00:00:00+00:00</updated>
      <id>/Virtual-Classrooms-and-Open-Book-Examinations</id>
      <content type="html" xml:base="/Virtual-Classrooms-and-Open-Book-Examinations">&lt;p&gt;Virtual classrooms is a phrase that has become an important part of the new normal, and along with it comes the infamous open-book examinations. Who would have thought that in 2020, the mainstream physical examinations would be replaced by open book examinations. But then again, who in their wildest dreams would have thought that the whole world will be stuck inside their homes, or away from it, for almost a year. The pandemic has clearly uprooted the lives of many, and the students, all across the globe, are no different.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Students have been forced to stay within the four walls of their houses, and attend digital schooling. In a country like India or the UK, it was March or April when the lockdown started as a result of the pandemic, and honestly, at that time no one thought it would go on for this long. In fact, no one was prepared for this pandemic and surely wasn’t ready to adapt to it.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;It was clear from the start that the educational institutions were going to make decisions that might not be suited for the heterogeneous society that we are. Too many debates, discussions and uncertainty led to all sorts of discussions on what to do for the students who had their exams approaching. Thus, emerged the idea of open-book examinations for final year college students. This was initially taken up by Delhi University. The exams got postponed so many times that by the time we took the test we had lost the count. But when these examinations did take place, they happened at a definite, not-so-unclear cost.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;You see, as a person, who could afford to have a separate room and a laptop to take the exams in peace, I was still drowned in intense amounts of anxiety. For, as a student since the past 17 years, all I had to focus on was my paper. Anything from a wrong question, to doubts, to submission of sheets was to be taken care of by the invigilators. The usage of any sort of technology wasn't even a part of the discussion. So these 17 years taught me to dedicate my whole three hours to write the exam. But with this pandemic and open-book examinations, it meant that for once I was required to do all the tasks by myself, using technology, an approach that many of us were clueless about, the added stress being whether the sheets were being uploaded and were the scanned images even clear. All this made me wonder about those who did not have all these layers of privilege I had, and the added stress levels they had to deal with. Obviously, by no means can I even come close to understanding how dreadful all this would have been for them.&lt;/p&gt;
&lt;p&gt;It is in times like these where the only source of interaction and schooling is through technology, people are forced to be left out, just because they cannot afford such a luxury. Thus, in such situations, something like education that we consider a necessity, in fact, becomes a luxury for many people.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Since the past few months, all the classes are being taken online. Within the span of these months, the mode of examinations has expanded to become MCQ-based or assignment-based. Despite all of this, looking at the state of students, it all doesn't look so different from the idea of a zero academic year, which many people find absurd. The students are constantly stuck onto the screens to study and then to do their homework and assignments, no wonder they are so tired from it all. From teachers figuring out how to make it all understandable to a class with no video and microphone on, to the students attending the class but sleeping through most of it. There is little to no education taking place.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;All of this seems like it was bound to happen because our teaching methods have been developed for face-to-face interactions and not for video-call sessions. Moreover, Indian education systems aren't even reformed enough to focus on teaching the concepts, rather than focusing on exams, which clearly doesn't sit well with the condition we are in currently. Hence, it really makes no sense to have open-book examinations or any form of virtual examinations for that matter, when our whole education system is based on seeing how well you remember what you have learnt, and not really how well you grasp the concept.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;The pandemic has already increased our anxiety levels, and an education like this just adds to it all. This is only the state of the ones who are privileged enough to actually have access to such resources and means of education. There are so many students out there who due to different reasons cannot take up online classes. And hence have no other option but to skip this year, in the hopes that they'll get to join next year. No one can imagine the stress of these children and the huge loss to their mental health as a result of not being able to take up classes this year. We did see one such incident come forth from Lady Shri Ram College, Delhi University, where a brilliant student was so burdened by the stress of her family's financial incapability that it made her take her own life. It was only then that the college decided to take action towards reducing fees and providing help to those in need. But is it really the lack of financial capability of the family, or the lack of concern of the people sitting on the other end, who refuse to look into how the pandemic could be affecting the students and their families?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Looking around we do see differences when it comes to how the private institutions and government institutions have dealt with digital schooling. And definitely, there is a clear difference between nations across the globe. However, despite these differences, with some institutions clearly having a better grasp of virtual classroom systems, none of the institutions were prepared to shift their entire curriculum fully online for a whole year, or maybe even more. And even though, for once I am willing to believe that all the educational institutions took the most suitable option in the current scenario, there are still aspects which they just chose to ignore. While they adapted to the new normal, they forgot to look into the students who too had to adapt to it. From lack of concern towards the cash crunch that people are facing, to the effects of the pandemic on the mental and physical well-being of students — all of this is far from being discussed. There are still students left who haven't taken examinations due to various reasons, and they are constantly under stress, with the uncertainty of when they'll actually take them only growing. Every day my Instagram stories are filled with students crying over the extensive amount of pressure from the deadlines of assignments and the upcoming exams.&lt;/p&gt;
&lt;p&gt;All of this makes us wonder what is the point of such an education that only focuses on finishing the curriculum and not even on making sure that each and every student is actually able to attend these classes. Even if it seems that digital schooling is the only possible option right now, is it really worth it if it is at the expense of the sanity of the students, especially with so many students being left out of it altogether?&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Priyanshi Mehra</name>
        
        
      </author>

      

      
        <category term="society" />
      

      
        <summary type="html">Virtual classrooms is a phrase that has become an important part of the new normal, and along with it comes the infamous open-book examinations. Who would have thought that in 2020, the mainstream physical examinations would be replaced by open book examinations. But then again, who in their wildest dreams would have thought that the whole world will be stuck inside their homes, or away from it, for almost a year. The pandemic has clearly uprooted the lives of many, and the students, all across the globe, are no different.&amp;nbsp; Students have been forced to stay within the four walls of their houses, and attend digital schooling. In a country like India or the UK, it was March or April when the lockdown started as a result of the pandemic, and honestly, at that time no one thought it would go on for this long. In fact, no one was prepared for this pandemic and surely wasn’t ready to adapt to it.&amp;nbsp; It was clear from the start that the educational institutions were going to make decisions that might not be suited for the heterogeneous society that we are. Too many debates, discussions and uncertainty led to all sorts of discussions on what to do for the students who had their exams approaching. Thus, emerged the idea of open-book examinations for final year college students. This was initially taken up by Delhi University. The exams got postponed so many times that by the time we took the test we had lost the count. But when these examinations did take place, they happened at a definite, not-so-unclear cost.&amp;nbsp; You see, as a person, who could afford to have a separate room and a laptop to take the exams in peace, I was still drowned in intense amounts of anxiety. For, as a student since the past 17 years, all I had to focus on was my paper. Anything from a wrong question, to doubts, to submission of sheets was to be taken care of by the invigilators. The usage of any sort of technology wasn't even a part of the discussion. So these 17 years taught me to dedicate my whole three hours to write the exam. But with this pandemic and open-book examinations, it meant that for once I was required to do all the tasks by myself, using technology, an approach that many of us were clueless about, the added stress being whether the sheets were being uploaded and were the scanned images even clear. All this made me wonder about those who did not have all these layers of privilege I had, and the added stress levels they had to deal with. Obviously, by no means can I even come close to understanding how dreadful all this would have been for them. It is in times like these where the only source of interaction and schooling is through technology, people are forced to be left out, just because they cannot afford such a luxury. Thus, in such situations, something like education that we consider a necessity, in fact, becomes a luxury for many people.&amp;nbsp; Since the past few months, all the classes are being taken online. Within the span of these months, the mode of examinations has expanded to become MCQ-based or assignment-based. Despite all of this, looking at the state of students, it all doesn't look so different from the idea of a zero academic year, which many people find absurd. The students are constantly stuck onto the screens to study and then to do their homework and assignments, no wonder they are so tired from it all. From teachers figuring out how to make it all understandable to a class with no video and microphone on, to the students attending the class but sleeping through most of it. There is little to no education taking place.&amp;nbsp; All of this seems like it was bound to happen because our teaching methods have been developed for face-to-face interactions and not for video-call sessions. Moreover, Indian education systems aren't even reformed enough to focus on teaching the concepts, rather than focusing on exams, which clearly doesn't sit well with the condition we are in currently. Hence, it really makes no sense to have open-book examinations or any form of virtual examinations for that matter, when our whole education system is based on seeing how well you remember what you have learnt, and not really how well you grasp the concept.&amp;nbsp; The pandemic has already increased our anxiety levels, and an education like this just adds to it all. This is only the state of the ones who are privileged enough to actually have access to such resources and means of education. There are so many students out there who due to different reasons cannot take up online classes. And hence have no other option but to skip this year, in the hopes that they'll get to join next year. No one can imagine the stress of these children and the huge loss to their mental health as a result of not being able to take up classes this year. We did see one such incident come forth from Lady Shri Ram College, Delhi University, where a brilliant student was so burdened by the stress of her family's financial incapability that it made her take her own life. It was only then that the college decided to take action towards reducing fees and providing help to those in need. But is it really the lack of financial capability of the family, or the lack of concern of the people sitting on the other end, who refuse to look into how the pandemic could be affecting the students and their families?&amp;nbsp; Looking around we do see differences when it comes to how the private institutions and government institutions have dealt with digital schooling. And definitely, there is a clear difference between nations across the globe. However, despite these differences, with some institutions clearly having a better grasp of virtual classroom systems, none of the institutions were prepared to shift their entire curriculum fully online for a whole year, or maybe even more. And even though, for once I am willing to believe that all the educational institutions took the most suitable option in the current scenario, there are still aspects which they just chose to ignore. While they adapted to the new normal, they forgot to look into the students who too had to adapt to it. From lack of concern towards the cash crunch that people are facing, to the effects of the pandemic on the mental and physical well-being of students — all of this is far from being discussed. There are still students left who haven't taken examinations due to various reasons, and they are constantly under stress, with the uncertainty of when they'll actually take them only growing. Every day my Instagram stories are filled with students crying over the extensive amount of pressure from the deadlines of assignments and the upcoming exams. All of this makes us wonder what is the point of such an education that only focuses on finishing the curriculum and not even on making sure that each and every student is actually able to attend these classes. Even if it seems that digital schooling is the only possible option right now, is it really worth it if it is at the expense of the sanity of the students, especially with so many students being left out of it altogether?</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Retail Binge for the Mental Twinge</title>
      <link href="/Retail-Binge-for-the-Mental-Twinge" rel="alternate" type="text/html" title="Retail Binge for the Mental Twinge" />
      <published>2020-12-03T00:00:00+00:00</published>
      <updated>2020-12-03T00:00:00+00:00</updated>
      <id>/Retail-Binge-for-the-Mental-Twinge</id>
      <content type="html" xml:base="/Retail-Binge-for-the-Mental-Twinge">&lt;p class=&quot;ql-align-justify&quot;&gt;They say, “When the going gets tough, the tough go shopping.” This is more commonly known as &lt;em&gt;Retail Therapy&lt;/em&gt;. It is a phenomenon where people shop to improve one’s disposition and raise spirits. People in distress often resort to this as a source of solace. A study published in the journal &lt;em&gt;Psychology and Marketing&lt;/em&gt; states that retail therapy has a lasting positive effect on mood. While its findings pointed out that these are impulsive purchases, the survey suggested that feelings of guilt or regret were not associated among the respondents.&amp;nbsp;The name in itself is ironic. Shopping is a concept deeply rooted in materialism. As its positive impact is temporary, it does not qualify as a ‘therapy’ in the medical sense. However, it is a fairly popular phrase used around the world.&amp;nbsp;&lt;/p&gt;
&lt;p class=&quot;ql-align-justify&quot;&gt;In a 2001 survey conducted by the European Union, it was found that 33% of the shoppers surveyed had a “high level of addiction to rash buying”. The study also revealed that ‘binge purchasing’ was most common among the Scottish youth. Another study of 1,000 American adults, conducted in 2013, detected that more than half of the respondents indulged in retail therapy. It also threw light on the gender aspect of the practice, stating that it is more common among women than men. It was found that 64.9% of women and 38.9% of men binge purchased while women mostly bought clothing, men indulged in ‘comfort food’. This was further reinforced by the Youngstown State University with similar male-female percentages, showing that relief from stress was the most common reason behind the practice. A more adverse extension of retail therapy is Oniomania or Compulsive Buying Disorder, which is an obsession with shopping, resulting in serious financial and mental consequences.&amp;nbsp;&lt;/p&gt;
&lt;p class=&quot;ql-align-justify&quot;&gt;&lt;strong&gt;The Whys behind the Comfort Buys&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;ql-align-justify&quot;&gt;While stress, anxiety and sadness are a part of daily life, they can be tackled through other means such as reaching out to others, practicing yoga, mindfulness and meditation, journaling, exercising and consulting experts in more severe cases. Despite there being these solutions, which on an average are cheaper than shopping, why do people indulge in purchasing things to boost their mood? To understand the causes behind retail therapy, the behavioral aspect of individuals needs to be analysed in a twofold manner. Even though, at first glance, these reasons seem simple — associated with daily life, a deeper analysis suggests that the root cause can be traced back to psychological theory.&amp;nbsp;&lt;/p&gt;
&lt;p class=&quot;ql-align-justify&quot;&gt;Firstly, we examine the relatively straightforward causes.&amp;nbsp;&amp;nbsp;When everything goes wrong and nothing falls into place as you imagined it to, how do you feel? Out of control, right? Thus, one of the reasons people indulge in retail therapy is that shopping is a coping mechanism that helps people feel more in control. From deciding where to shop, to choosing what to purchase, it allows people to exercise autonomy and make decisions. In my personal opinion, shopping can also make one feel a sense of achievement. Utilising good offers and deals, applying coupons to get discounts and sometimes even getting goods for free certainly makes one feel happier as they ‘save’ along with fulfilling a guilty pleasure.&amp;nbsp;&lt;/p&gt;
&lt;p class=&quot;ql-align-justify&quot;&gt;Secondly, retail therapy can be a useful mechanism for smooth transitions of life as it enables people to envisage their lives post such changes. For instance, people often shop excessively before they are going to live alone for the first time, shifting to a new house, getting married or having a baby. Shopping may ease anxiety by making people feel more in control by helping them to prepare themselves for such huge transitions through the process of ‘visualisation’, which is a process that boosts performance, confidence and reduces anxiety.&amp;nbsp;&lt;/p&gt;
&lt;p class=&quot;ql-align-justify&quot;&gt;Another way in which retail therapy boosts mood is through a pleasant gush of creativity. Specifically, clothing of various kinds of colors and textures, accessories like jewelry, footwear, bags, sunglasses, etc, home decor items may induce a positive breeze of art and aesthetics. For example, the arrival of a new outfit may stir your creative juices in thinking about what shoes, jewelry and bag to pair it up with. This may certainly bring about a calmness in a way similar to art therapy, a well-known method of healing and reducing stress.&amp;nbsp;&lt;/p&gt;
&lt;p class=&quot;ql-align-justify&quot;&gt;Some people treat shopping, both online and offline as a source of refreshment from the humdrum of daily life. Being a mindless activity, it enables people to relax and take some time off from their sources of stress. This form may vary from window shopping to scrolling on an online website or going to a shopping-mall for a break.&amp;nbsp;Yet another reason behind binge-purchasing may be the need to ‘feel connected’. For instance, one may shop for a sweater with a snowman or bells, candies and wreaths for decoration during Christmas, to feel more socially connected to the festival. Or someone may shop for a beret before visiting Paris to feel a part of the culture of the fashion capital. Thus, retail therapy may help individuals to feel closer to society and culture.&amp;nbsp;&lt;/p&gt;
&lt;p class=&quot;ql-align-justify&quot;&gt;An extensive scrutiny of the idea suggests that the root cause lies in Sigmund Freud’s Pleasure Principle of psychology. The principle claims that individuals instinctively seek immediate pleasure to avoid experiencing feelings of stress, pain and sadness. In the context of impulsive purchasing, the principle fits well, as it is only a short term coping mechanism rooted in instant relief from stress. As compared to the Freudian Reality Principle, the Pleasure Principle is the &lt;a href=&quot;https://thepangean.com/Present-Bias-Focus-on-the-Now-but-maybe-Not-Always&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;inability of individuals to defer immediate gratification of certain desires and wants that rationality and reality may disallow.&amp;nbsp;&lt;/a&gt;&lt;/p&gt;
&lt;p class=&quot;ql-align-justify&quot;&gt;&lt;strong&gt;The Sadness Cycle&amp;nbsp;&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;ql-align-justify&quot;&gt;Even though retail therapy eases the anxiety and pain in people’s lives, such a positive impact may be short-lived. While retail therapy can be supported by psychological reasoning, Economics does not go hand-in-hand with the concept due to its assumption of rationality among all individuals. From the point of view of rationality, the problems of running out of budget and debts arise due to limited financial resources. Impulsive overspending may lead people to become even more anxious and distressed due to the inability of paying bills and lacking a sound financial base, in the long run. A rough patch after having run out of budget may seem more dramatic and difficult to handle, due to a lack of finance to purchase something to feel better. In desperate times like these, individuals addicted to the method, may resort to desperate measures like taking loans from others or stealing. This might be complemented with feelings of guilt due to unrealistically high expenditure patterns.&amp;nbsp;&lt;/p&gt;
&lt;p class=&quot;ql-align-justify&quot;&gt;Thus, retail therapy, to an obsessive degree can lead to reverse effects by introducing a cycle of stress and anxiety for an individual rather than helping one fight the tough times.&amp;nbsp;&lt;/p&gt;
&lt;p class=&quot;ql-align-justify&quot;&gt;&lt;strong&gt;The ‘Viral’ Remedy&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;
&lt;p class=&quot;ql-align-justify&quot;&gt;With the Coronavirus pandemic hitting the world, people across the globe felt paralysed due to being restricted homes all the time. While this altered the work culture among people, the norms of schooling among students and put a halt to dining out, it had an impact on the shoppers. Research suggests that elements appealing to an individual’s senses, like aroma, music, a creative arrangement of products in a store, temperature contribute to the experience of shoppers. Unfortunately, with the lockdown implemented across the world, this ‘sensory’ experience was no longer available. As a result of this, one would expect sales to fall. However, the online retail industry led to a different turn of events.&lt;/p&gt;
&lt;p class=&quot;ql-align-justify&quot;&gt;According to &lt;em&gt;Forbes&lt;/em&gt;, as of March 2020, several online retail categories experienced a 74% year over year increase in sales, as compared to March 2018. Some of the top categories that saw a surge in the sale are home goods like furniture and home decor items and athletic and loungewear. While it may be argued that these categories resonate more with individuals in the current scenario due to the need to stay home, it must be noted that none of these items are essentials. Thus, it can be suggested that individuals binge purchased during the pandemic to escape the fears and stress of the situation and its uncertainty, thereby serving as a recent instance of retail therapy.&amp;nbsp;&lt;/p&gt;
&lt;p class=&quot;ql-align-justify&quot;&gt;The impact of retail therapy on an individual’s life depends upon the degree of engagement. On one hand, retail therapy in the form of window shopping, purchasing with adherence to one’s budget or buying pre-planned things can help to reduce stress and sadness both in the short term and long run. On the other hand, if the simple habit intensifies to become an addiction, it results in adverse problems which in turn brings more distress, thereby defeating the purpose of the ‘therapy’.&amp;nbsp;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Nirikta Mukherjee</name>
        
        
      </author>

      

      
        <category term="society" />
      

      
        <summary type="html">They say, “When the going gets tough, the tough go shopping.” This is more commonly known as Retail Therapy. It is a phenomenon where people shop to improve one’s disposition and raise spirits. People in distress often resort to this as a source of solace. A study published in the journal Psychology and Marketing states that retail therapy has a lasting positive effect on mood. While its findings pointed out that these are impulsive purchases, the survey suggested that feelings of guilt or regret were not associated among the respondents.&amp;nbsp;The name in itself is ironic. Shopping is a concept deeply rooted in materialism. As its positive impact is temporary, it does not qualify as a ‘therapy’ in the medical sense. However, it is a fairly popular phrase used around the world.&amp;nbsp; In a 2001 survey conducted by the European Union, it was found that 33% of the shoppers surveyed had a “high level of addiction to rash buying”. The study also revealed that ‘binge purchasing’ was most common among the Scottish youth. Another study of 1,000 American adults, conducted in 2013, detected that more than half of the respondents indulged in retail therapy. It also threw light on the gender aspect of the practice, stating that it is more common among women than men. It was found that 64.9% of women and 38.9% of men binge purchased while women mostly bought clothing, men indulged in ‘comfort food’. This was further reinforced by the Youngstown State University with similar male-female percentages, showing that relief from stress was the most common reason behind the practice. A more adverse extension of retail therapy is Oniomania or Compulsive Buying Disorder, which is an obsession with shopping, resulting in serious financial and mental consequences.&amp;nbsp; The Whys behind the Comfort Buys&amp;nbsp; While stress, anxiety and sadness are a part of daily life, they can be tackled through other means such as reaching out to others, practicing yoga, mindfulness and meditation, journaling, exercising and consulting experts in more severe cases. Despite there being these solutions, which on an average are cheaper than shopping, why do people indulge in purchasing things to boost their mood? To understand the causes behind retail therapy, the behavioral aspect of individuals needs to be analysed in a twofold manner. Even though, at first glance, these reasons seem simple — associated with daily life, a deeper analysis suggests that the root cause can be traced back to psychological theory.&amp;nbsp; Firstly, we examine the relatively straightforward causes.&amp;nbsp;&amp;nbsp;When everything goes wrong and nothing falls into place as you imagined it to, how do you feel? Out of control, right? Thus, one of the reasons people indulge in retail therapy is that shopping is a coping mechanism that helps people feel more in control. From deciding where to shop, to choosing what to purchase, it allows people to exercise autonomy and make decisions. In my personal opinion, shopping can also make one feel a sense of achievement. Utilising good offers and deals, applying coupons to get discounts and sometimes even getting goods for free certainly makes one feel happier as they ‘save’ along with fulfilling a guilty pleasure.&amp;nbsp; Secondly, retail therapy can be a useful mechanism for smooth transitions of life as it enables people to envisage their lives post such changes. For instance, people often shop excessively before they are going to live alone for the first time, shifting to a new house, getting married or having a baby. Shopping may ease anxiety by making people feel more in control by helping them to prepare themselves for such huge transitions through the process of ‘visualisation’, which is a process that boosts performance, confidence and reduces anxiety.&amp;nbsp; Another way in which retail therapy boosts mood is through a pleasant gush of creativity. Specifically, clothing of various kinds of colors and textures, accessories like jewelry, footwear, bags, sunglasses, etc, home decor items may induce a positive breeze of art and aesthetics. For example, the arrival of a new outfit may stir your creative juices in thinking about what shoes, jewelry and bag to pair it up with. This may certainly bring about a calmness in a way similar to art therapy, a well-known method of healing and reducing stress.&amp;nbsp; Some people treat shopping, both online and offline as a source of refreshment from the humdrum of daily life. Being a mindless activity, it enables people to relax and take some time off from their sources of stress. This form may vary from window shopping to scrolling on an online website or going to a shopping-mall for a break.&amp;nbsp;Yet another reason behind binge-purchasing may be the need to ‘feel connected’. For instance, one may shop for a sweater with a snowman or bells, candies and wreaths for decoration during Christmas, to feel more socially connected to the festival. Or someone may shop for a beret before visiting Paris to feel a part of the culture of the fashion capital. Thus, retail therapy may help individuals to feel closer to society and culture.&amp;nbsp; An extensive scrutiny of the idea suggests that the root cause lies in Sigmund Freud’s Pleasure Principle of psychology. The principle claims that individuals instinctively seek immediate pleasure to avoid experiencing feelings of stress, pain and sadness. In the context of impulsive purchasing, the principle fits well, as it is only a short term coping mechanism rooted in instant relief from stress. As compared to the Freudian Reality Principle, the Pleasure Principle is the inability of individuals to defer immediate gratification of certain desires and wants that rationality and reality may disallow.&amp;nbsp; The Sadness Cycle&amp;nbsp;&amp;nbsp; Even though retail therapy eases the anxiety and pain in people’s lives, such a positive impact may be short-lived. While retail therapy can be supported by psychological reasoning, Economics does not go hand-in-hand with the concept due to its assumption of rationality among all individuals. From the point of view of rationality, the problems of running out of budget and debts arise due to limited financial resources. Impulsive overspending may lead people to become even more anxious and distressed due to the inability of paying bills and lacking a sound financial base, in the long run. A rough patch after having run out of budget may seem more dramatic and difficult to handle, due to a lack of finance to purchase something to feel better. In desperate times like these, individuals addicted to the method, may resort to desperate measures like taking loans from others or stealing. This might be complemented with feelings of guilt due to unrealistically high expenditure patterns.&amp;nbsp; Thus, retail therapy, to an obsessive degree can lead to reverse effects by introducing a cycle of stress and anxiety for an individual rather than helping one fight the tough times.&amp;nbsp; The ‘Viral’ Remedy&amp;nbsp; With the Coronavirus pandemic hitting the world, people across the globe felt paralysed due to being restricted homes all the time. While this altered the work culture among people, the norms of schooling among students and put a halt to dining out, it had an impact on the shoppers. Research suggests that elements appealing to an individual’s senses, like aroma, music, a creative arrangement of products in a store, temperature contribute to the experience of shoppers. Unfortunately, with the lockdown implemented across the world, this ‘sensory’ experience was no longer available. As a result of this, one would expect sales to fall. However, the online retail industry led to a different turn of events. According to Forbes, as of March 2020, several online retail categories experienced a 74% year over year increase in sales, as compared to March 2018. Some of the top categories that saw a surge in the sale are home goods like furniture and home decor items and athletic and loungewear. While it may be argued that these categories resonate more with individuals in the current scenario due to the need to stay home, it must be noted that none of these items are essentials. Thus, it can be suggested that individuals binge purchased during the pandemic to escape the fears and stress of the situation and its uncertainty, thereby serving as a recent instance of retail therapy.&amp;nbsp; The impact of retail therapy on an individual’s life depends upon the degree of engagement. On one hand, retail therapy in the form of window shopping, purchasing with adherence to one’s budget or buying pre-planned things can help to reduce stress and sadness both in the short term and long run. On the other hand, if the simple habit intensifies to become an addiction, it results in adverse problems which in turn brings more distress, thereby defeating the purpose of the ‘therapy’.&amp;nbsp;</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Present Bias: Focus on the Now but maybe Not Always</title>
      <link href="/Present-Bias-Focus-on-the-Now-but-maybe-Not-Always" rel="alternate" type="text/html" title="Present Bias: Focus on the Now but maybe Not Always" />
      <published>2020-11-21T00:00:00+00:00</published>
      <updated>2020-11-21T00:00:00+00:00</updated>
      <id>/Present-Bias-Focus-on-the-Now-but-maybe-Not-Always</id>
      <content type="html" xml:base="/Present-Bias-Focus-on-the-Now-but-maybe-Not-Always">&lt;p&gt;Many times we wonder, what is it that makes us procrastinate? Why do we spend the whole day scrolling through Instagram instead of finishing the assignment that is due tomorrow? Apart from the fact that social media is designed to distract us, procrastinating isn't as simple as just being lazy. It isn't only about not wanting to do the task at hand, but more about how we humans are wired.&amp;nbsp; Procrastination is in fact related to a cognitive bias called the &lt;em&gt;present bias&lt;/em&gt;. As the name in itself suggests, it is the tendency of humans to be biased towards the present. This means that we tend to go for short-term rewards that provide us instant gratification rather than choosing long-term benefits. Thus, even though the benefit of finishing that assignment is clearly more than simply being on Instagram, the sense of gratification is achieved faster in the latter. Therefore, our brain might as well trick us into choosing the latter.&lt;/p&gt;
&lt;p&gt;It is important to understand why humans are actually biased towards the present. The answer lies in how our ancestors lived their lives. The times they lived in, it only made sense that they focused on the present. With the uncertainty of the future and the environment they lived in, they had no other option but to be short-sighted in their decision making. This present bias allowed them to survive and deal with the immediate threats that existed around them. Over the centuries, this outlook of theirs seems to have been so well engraved in our brains that even today we have a similar mindset. Despite us having a far more stable lifestyle and a future to actually look forward to, our brain still makes us biased towards our present. It still makes us believe that the future is a scary avenue that one shouldn't be focusing much on it.&lt;/p&gt;
&lt;p&gt;The idea of the present bias has been studied by many researchers over the years, in order to understand to what extent such a bias can affect the lives of individuals. An interesting experiment in this field was conducted by Walter Mischel. This experiment that goes by the name of &lt;em&gt;Marshmallow Test &lt;/em&gt;is a famous experiment which involved finding out whether children delayed gratification. It involved children being asked to either choose a marshmallow now or wait for 15-20 minutes to get two of them. This experiment, however, did not end here only. After a few years, these very children, who were by then adolescents, were traced and it was seen that the children who delayed gratification at a younger age were able to be more successful in the later period of their lives. Thus, shedding light on the connection between small decisions turning into a lifestyle.&lt;/p&gt;
&lt;p&gt;An interesting aspect of present bias is that it is not just limited to the field of psychology but also equally important for economics. Take for an example, if you were to choose between getting a job of ₹15,000/month now, or interning for free for a year and getting a job of ₹30,000/month. You are more likely to choose the former. But if you were asked to either complete your 3 years of Bachelor's and get a job starting at ₹15,000/month or to choose an integrated programme of 4 years that would allow you to get a package starting at ₹30,000/month, then you are most likely to choose the second option. Even though there isn’t a difference between these two cases other than the time periods, your choice would potentially differ. This is because our brain can only distinguish between the present and the future.&lt;/p&gt;
&lt;p&gt;Whether the future is three or four years down the lane, our brain fails to distinguish between them, because both are considered as far-sighted. Since our mind takes the future as uncertain it wants to make decisions for now, even though they might not be as beneficial. In economics, present bias is used when we talk about time inconsistency and discounting. Time inconsistency refers to the fact that individuals are inconsistent over time. That is, for instance, you decide to hit the gym next month, but when next month arrives, you are more likely to postpone it to another month. Thus, meaning that people aren't as rational in making decisions, but there are various social and psychological factors that govern their decisions. People have the tendency to change their choices, over different time periods even though the benefits remain the same. Richard H Thaler (a Nobel Prize Winner for his work in the field of behavioural economics) has conducted researches that show that economics and psychology are far more closely related than one can imagine, thus expanding the scope and importance of behavioural economics. He along with H M Shefrin wrote a paper titled &lt;em&gt;An Economic Theory of Self-control&lt;/em&gt; that introduced an economic model of self-control. This model explains that people aren't only lacking self-control when it comes to not buying new clothes, or when it comes to stopping with just another episode. But the lack of self-control can also be seen towards how people handle their finances, especially saving for their retirements before it gets late. To us, saving money seems more like a loss now even though it’ll benefit us in future, and thus people tend to postpone saving money. In fact, this research has led to various social policies to emerge to help people with saving their money from early days.&lt;/p&gt;
&lt;p&gt;Since it is wired into our system to focus on what could harm us immediately, rather than what could harm us in the long-term, present bias manifests itself in many of our actions. The concept of present bias explains why we humans tend to make self-harming choices that provide us instant gratification but clearly have harmful consequences in the future. It also explains why we tend to ignore the importance of climate change, since it doesn't seem like an instant harm to our minds.&lt;/p&gt;
&lt;p&gt;Living in the present is important, but it isn't all we should be focused on. In reality, our lives are a combination of the past, present and future and therefore it becomes equally important to realise that the present is only a small piece in the puzzle of life. Understanding what present bias is, and how strongly it can control our decision making, allows us to be more aware of our own selves. It makes us realise that our brains aren't always thinking what's best for us and can quite a few times make impulsive decisions that might not seem so right in the years to come. Clearly, our brains are guided by the fear of the future, because of all the uncertainty it brings. So it only makes sense that we continue to ignore the future, and be in a more comfortable zone, the present. And honestly, instant gratification, no matter for how little of a time, does feel good. But it also feels fulfilling to make decisions that'll help our future selves. This is the dilemma that present bias poses. Whether to choose the time that we know of or to choose the time that'll come. It isn't easy to break away from our habits, especially the ones that are so well engraved in our nerves. Self-control and will power aren't as easily attained as we are likely to believe. But what we can do is to be more aware of how our brains have a tendency of fooling our own selves, and maybe for once not trust whatever our brain has to say.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Priyanshi Mehra</name>
        
        
      </author>

      

      
        <category term="society" />
      

      
        <summary type="html">Many times we wonder, what is it that makes us procrastinate? Why do we spend the whole day scrolling through Instagram instead of finishing the assignment that is due tomorrow? Apart from the fact that social media is designed to distract us, procrastinating isn't as simple as just being lazy. It isn't only about not wanting to do the task at hand, but more about how we humans are wired.&amp;nbsp; Procrastination is in fact related to a cognitive bias called the present bias. As the name in itself suggests, it is the tendency of humans to be biased towards the present. This means that we tend to go for short-term rewards that provide us instant gratification rather than choosing long-term benefits. Thus, even though the benefit of finishing that assignment is clearly more than simply being on Instagram, the sense of gratification is achieved faster in the latter. Therefore, our brain might as well trick us into choosing the latter. It is important to understand why humans are actually biased towards the present. The answer lies in how our ancestors lived their lives. The times they lived in, it only made sense that they focused on the present. With the uncertainty of the future and the environment they lived in, they had no other option but to be short-sighted in their decision making. This present bias allowed them to survive and deal with the immediate threats that existed around them. Over the centuries, this outlook of theirs seems to have been so well engraved in our brains that even today we have a similar mindset. Despite us having a far more stable lifestyle and a future to actually look forward to, our brain still makes us biased towards our present. It still makes us believe that the future is a scary avenue that one shouldn't be focusing much on it. The idea of the present bias has been studied by many researchers over the years, in order to understand to what extent such a bias can affect the lives of individuals. An interesting experiment in this field was conducted by Walter Mischel. This experiment that goes by the name of Marshmallow Test is a famous experiment which involved finding out whether children delayed gratification. It involved children being asked to either choose a marshmallow now or wait for 15-20 minutes to get two of them. This experiment, however, did not end here only. After a few years, these very children, who were by then adolescents, were traced and it was seen that the children who delayed gratification at a younger age were able to be more successful in the later period of their lives. Thus, shedding light on the connection between small decisions turning into a lifestyle. An interesting aspect of present bias is that it is not just limited to the field of psychology but also equally important for economics. Take for an example, if you were to choose between getting a job of ₹15,000/month now, or interning for free for a year and getting a job of ₹30,000/month. You are more likely to choose the former. But if you were asked to either complete your 3 years of Bachelor's and get a job starting at ₹15,000/month or to choose an integrated programme of 4 years that would allow you to get a package starting at ₹30,000/month, then you are most likely to choose the second option. Even though there isn’t a difference between these two cases other than the time periods, your choice would potentially differ. This is because our brain can only distinguish between the present and the future. Whether the future is three or four years down the lane, our brain fails to distinguish between them, because both are considered as far-sighted. Since our mind takes the future as uncertain it wants to make decisions for now, even though they might not be as beneficial. In economics, present bias is used when we talk about time inconsistency and discounting. Time inconsistency refers to the fact that individuals are inconsistent over time. That is, for instance, you decide to hit the gym next month, but when next month arrives, you are more likely to postpone it to another month. Thus, meaning that people aren't as rational in making decisions, but there are various social and psychological factors that govern their decisions. People have the tendency to change their choices, over different time periods even though the benefits remain the same. Richard H Thaler (a Nobel Prize Winner for his work in the field of behavioural economics) has conducted researches that show that economics and psychology are far more closely related than one can imagine, thus expanding the scope and importance of behavioural economics. He along with H M Shefrin wrote a paper titled An Economic Theory of Self-control that introduced an economic model of self-control. This model explains that people aren't only lacking self-control when it comes to not buying new clothes, or when it comes to stopping with just another episode. But the lack of self-control can also be seen towards how people handle their finances, especially saving for their retirements before it gets late. To us, saving money seems more like a loss now even though it’ll benefit us in future, and thus people tend to postpone saving money. In fact, this research has led to various social policies to emerge to help people with saving their money from early days. Since it is wired into our system to focus on what could harm us immediately, rather than what could harm us in the long-term, present bias manifests itself in many of our actions. The concept of present bias explains why we humans tend to make self-harming choices that provide us instant gratification but clearly have harmful consequences in the future. It also explains why we tend to ignore the importance of climate change, since it doesn't seem like an instant harm to our minds. Living in the present is important, but it isn't all we should be focused on. In reality, our lives are a combination of the past, present and future and therefore it becomes equally important to realise that the present is only a small piece in the puzzle of life. Understanding what present bias is, and how strongly it can control our decision making, allows us to be more aware of our own selves. It makes us realise that our brains aren't always thinking what's best for us and can quite a few times make impulsive decisions that might not seem so right in the years to come. Clearly, our brains are guided by the fear of the future, because of all the uncertainty it brings. So it only makes sense that we continue to ignore the future, and be in a more comfortable zone, the present. And honestly, instant gratification, no matter for how little of a time, does feel good. But it also feels fulfilling to make decisions that'll help our future selves. This is the dilemma that present bias poses. Whether to choose the time that we know of or to choose the time that'll come. It isn't easy to break away from our habits, especially the ones that are so well engraved in our nerves. Self-control and will power aren't as easily attained as we are likely to believe. But what we can do is to be more aware of how our brains have a tendency of fooling our own selves, and maybe for once not trust whatever our brain has to say.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">The Height of Oppression</title>
      <link href="/The-Height-of-Oppression" rel="alternate" type="text/html" title="The Height of Oppression" />
      <published>2020-11-19T00:00:00+00:00</published>
      <updated>2020-11-19T00:00:00+00:00</updated>
      <id>/The-Height-of-Oppression</id>
      <content type="html" xml:base="/The-Height-of-Oppression">&lt;p class=&quot;ql-align-justify&quot;&gt;Heels are most often associated with femininity, and as a result, generally imposed on women for the sake of keeping up with the imagery. Multiple versions of the heel have been invented, such as pumps, stilettos, kitten heels and many more, and it is expected of women to wear heels to most places outside of the casual settings – their workspaces, fashion runways and extending, but not limited, to parties of differing natures. Multiple ceremonies and awards functions have a requirement that women must wear heels, the Cannes Film Festival in France is one of the biggest examples of this; one of their rules regarding the dress code stipulates that all women attendees must wear high heels in order to attend the ever so prestigious film festival. Over the years, several women celebrities have been stopped from attending the film festival for not wearing high heels. Not only such events but many companies, office places and restaurants require female employees to wear heels. There have been several instances of discrimination against women for their refusal to wear heels. For example, one such case of discrimination occurred when Nicola Thorp was fired by her manager for refusing to wear heels on her first day of work at an office job in London back in 2016. She was dressed in a suit and a pair of formal shoes. Shortly after she was fired, Thorp gained worldwide support for challenging the manager by filing a petition. The government concurred and felt that she had been discriminated against for not wearing heels, which is unlawful.&lt;/p&gt;
&lt;p class=&quot;ql-align-justify&quot;&gt;Wearing heels has become common in this day and age, it is expected of women to wear exhaustingly high heels everywhere. But there is a question that arises, how much of this expectation to wear heels for formal occasions is justified? It is evident that women must have the freedom of choice to wear whichever footwear or clothing they feel most comfortable in. If a woman feels most comfortable while wearing heels, then that is a fully valid choice. But even in these progressive times, it is men who enjoy the very same freedom, i.e. the freedom of controlling what a woman has to wear in most situations. Most women do not expressly enjoy wearing heels every single day, the same footwear which can become extremely painful for a person to wear for a mere few hours. Aside from giving women a sense of authority and power while wearing heels, there seem to be no other evident benefits or signs of comfort for a woman to wear the same. The question arises, why do women continue wearing heels despite all the negative aspects, and there being no apparent need to? The answer to this lies in the oppression of women, which ultimately stems from the system of patriarchy which is persistent in remaining pervasive in society. In order to truly understand the reasons though, we must look at the history of the ‘heel,’ and the shift in society’s perspective regarding it.&amp;nbsp;&lt;/p&gt;
&lt;p class=&quot;ql-align-justify&quot;&gt;The history of heels can be traced back to the 3,500 BCE; where it was mostly the nobility who wore heels, as opposed to the common people who roamed around barefoot. Heels were also used for practical purposes, such as by butchers to ensure their feet did not come in contact with the blood or the meat of the animal. Heels were later on adopted by the Greeks and Romans to display the differences between different social classes, it is suggested that men belonging to the Middle East – who wore heels during wars while riding horses in order to get a better grip while shooting arrows – brought the concept of heels to Europe. Thus, European aristocracy soon thereafter realised that heels served a practical purpose of keeping their feet out of the mud. After all, it was mostly the higher classes who cared about protecting their feet from mud or dirt as a means to distinguish themselves from the lower classes whilst also serving a useful purpose at the same time. Both men and women had begun wearing heels around this time. Soon after the ‘chopines’ emerged, they were the first instance of platform heels that were invented. They were famous for being severely uncomfortable and hard to walk in, and often required the assistance of several men to help in walking. The chopines were the first heels that were specifically designed for women, but many feminists argue that they were invented to limit the movement of women and to ensure they do not leave their households as much. In addition to this, it became common practice to force Chinese concubines and Turkish odalisques to wear chopines in order to ensure that they would not flee. The very first example of men using heels to control the movement or freedom of women can be seen here.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Up and until the 17th century, heels were most commonly worn by men. In the 18th century, however, when the idea of imperialism had started to spread around the world, men started to abandon heels. By the 19th century, heels went on to be recognised as something inherently feminine. Post the Second World War, Christian Dior came up with ‘stilettos,’ in which he used steel rods, raising the height of the heels to three inches or more. Since then, a multitude of designers such as Manolo Blahnik, Jimmy Choo, Alexander Mcqueen, and Christian Louboutin have invented different versions of the heel – all men designers scrambled to create a more elevated and much more impractical version of the heel. Designers who are known for creating quality heels usually happen to be men; the irony of men creating such footwear meant especially for women cannot go unnoticed. High heels are known and are proven to be harmful for a woman’s posture and feet, yet there is a persistent buildup of undue pressure on women to continue wearing heels. Christian Louboutin, when asked to comment upon the fact that heels are uncomfortable to walk in and slow a woman down, said, “&lt;em&gt;What is the point of wanting to run?&lt;/em&gt;” he continued, “&lt;em&gt;I am all for the pace getting slower, and high heels are very good for that.&lt;/em&gt;” That's right, ‘no point of women moving at a fast pace in a man’s world’.&lt;/p&gt;
&lt;p class=&quot;ql-align-justify&quot;&gt;According to a study conducted by the &lt;em&gt;American Orthopaedic Foot and Ankle Society&lt;/em&gt;, 50% of women experience daily pain from wearing their heels. Heels often put pressure on not only the feet but also the back, spines, knees and toes. High and tightly worn heels also cause blisters for a lot. Aside from this, heels can cause a myriad of other long-term problems as well, such as nerve damage and sciatica. Still, a majority of women continue wearing heels – either by their own choice or otherwise. In most parties or companies, men only feel obligated to wear suits with formal shoes; whereas women feel obligated to wear a dress and high heels as it shows ‘professional etiquette’, and many times dressing professionally can be mistaken with dressing uncomfortably just to please others. Although, women choose to wear heels most of the time as it may hand them a sense of ‘power’ and ‘authority,’ (heels are treated with a sense of hyper feminisation) but in most cases, women feel the need to abide by the invisible aura of the social construct.&amp;nbsp;&lt;/p&gt;
&lt;p class=&quot;ql-align-justify&quot;&gt;Nowadays, due to the influence of the fashion industry that has capitalised off of the social construct created by the existence of heels for women, there is a shift or a transition in the way heels have started to be perceived by women in general - earlier, heels used to be treated as modes that were used by a very limited number of people whose usage of heels usually did not allow for free movement or allow for any comfort. Now, due to high-end designers coming up with more innovative heel designs and popularising the usage of heels, women perceive heels to be footwear that, when worn, can be empowering, bold, sexy and fashionable instead of simply being uncomfortable and constrictive.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;In 2013, Louboutin told Vogue, “&lt;em&gt;It's almost three people who are different in terms of character. I never forget that shoes also have to please men. As a man, I totally understand looking at a girl and saying, “Darling, we're having dinner together tonight, so do you mind to change?” I understand that type of mentality from a man who loves his wife, his woman. He's concerned about the way you look. It's not an ugly thing. I understand also that a woman could not care. I like my design to please women, but also to please men.&lt;/em&gt;”&amp;nbsp;&lt;/p&gt;
&lt;p&gt;His own words make it sound like women are meant to wear heels for ‘the male gaze,’ which seems to be the only somewhat reasonable explanation behind a woman wearing heels. This can be interpreted as women not having to be overly concerned with the way they look while wearing heels, it only matters as long as a man likes what his wife or girlfriend might be wearing.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;It has become extremely normal to see women wearing extremely high heels in their everyday lives – no matter how unbearable it may be to walk in them. There will most likely not be any shift in society regarding the same, women will continue wearing heels for a long time. What can be normalised though, is ensuring that women need not feel as though they need to adhere to a certain set of expectations from society. The shift from heels being used mostly by men to then being used solely by women is a drastic one; and before we all start feminising heels to such a large extent, we must remember where they originated from and which of the two sexes wore them in the beginning, and also helped in popularising them all around the world. Heels have had a sexist and controlling past, traces of which can be seen in today’s time as well. One can only hope that fashion does not aid in the widening of the gap between men and women. At the end of the day, we must remember that gendered clothing and footwear are not defined by which sex dons the piece the most, but it is defined by how people aside from that specific gender view it. Heels can be worn by any sex or gender, but it happens to be attached to femininity.&amp;nbsp;&amp;nbsp;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Pooja Bommareddy</name>
        
        
      </author>

      

      
        <category term="society" />
      

      
        <summary type="html">Heels are most often associated with femininity, and as a result, generally imposed on women for the sake of keeping up with the imagery. Multiple versions of the heel have been invented, such as pumps, stilettos, kitten heels and many more, and it is expected of women to wear heels to most places outside of the casual settings – their workspaces, fashion runways and extending, but not limited, to parties of differing natures. Multiple ceremonies and awards functions have a requirement that women must wear heels, the Cannes Film Festival in France is one of the biggest examples of this; one of their rules regarding the dress code stipulates that all women attendees must wear high heels in order to attend the ever so prestigious film festival. Over the years, several women celebrities have been stopped from attending the film festival for not wearing high heels. Not only such events but many companies, office places and restaurants require female employees to wear heels. There have been several instances of discrimination against women for their refusal to wear heels. For example, one such case of discrimination occurred when Nicola Thorp was fired by her manager for refusing to wear heels on her first day of work at an office job in London back in 2016. She was dressed in a suit and a pair of formal shoes. Shortly after she was fired, Thorp gained worldwide support for challenging the manager by filing a petition. The government concurred and felt that she had been discriminated against for not wearing heels, which is unlawful. Wearing heels has become common in this day and age, it is expected of women to wear exhaustingly high heels everywhere. But there is a question that arises, how much of this expectation to wear heels for formal occasions is justified? It is evident that women must have the freedom of choice to wear whichever footwear or clothing they feel most comfortable in. If a woman feels most comfortable while wearing heels, then that is a fully valid choice. But even in these progressive times, it is men who enjoy the very same freedom, i.e. the freedom of controlling what a woman has to wear in most situations. Most women do not expressly enjoy wearing heels every single day, the same footwear which can become extremely painful for a person to wear for a mere few hours. Aside from giving women a sense of authority and power while wearing heels, there seem to be no other evident benefits or signs of comfort for a woman to wear the same. The question arises, why do women continue wearing heels despite all the negative aspects, and there being no apparent need to? The answer to this lies in the oppression of women, which ultimately stems from the system of patriarchy which is persistent in remaining pervasive in society. In order to truly understand the reasons though, we must look at the history of the ‘heel,’ and the shift in society’s perspective regarding it.&amp;nbsp; The history of heels can be traced back to the 3,500 BCE; where it was mostly the nobility who wore heels, as opposed to the common people who roamed around barefoot. Heels were also used for practical purposes, such as by butchers to ensure their feet did not come in contact with the blood or the meat of the animal. Heels were later on adopted by the Greeks and Romans to display the differences between different social classes, it is suggested that men belonging to the Middle East – who wore heels during wars while riding horses in order to get a better grip while shooting arrows – brought the concept of heels to Europe. Thus, European aristocracy soon thereafter realised that heels served a practical purpose of keeping their feet out of the mud. After all, it was mostly the higher classes who cared about protecting their feet from mud or dirt as a means to distinguish themselves from the lower classes whilst also serving a useful purpose at the same time. Both men and women had begun wearing heels around this time. Soon after the ‘chopines’ emerged, they were the first instance of platform heels that were invented. They were famous for being severely uncomfortable and hard to walk in, and often required the assistance of several men to help in walking. The chopines were the first heels that were specifically designed for women, but many feminists argue that they were invented to limit the movement of women and to ensure they do not leave their households as much. In addition to this, it became common practice to force Chinese concubines and Turkish odalisques to wear chopines in order to ensure that they would not flee. The very first example of men using heels to control the movement or freedom of women can be seen here.&amp;nbsp; Up and until the 17th century, heels were most commonly worn by men. In the 18th century, however, when the idea of imperialism had started to spread around the world, men started to abandon heels. By the 19th century, heels went on to be recognised as something inherently feminine. Post the Second World War, Christian Dior came up with ‘stilettos,’ in which he used steel rods, raising the height of the heels to three inches or more. Since then, a multitude of designers such as Manolo Blahnik, Jimmy Choo, Alexander Mcqueen, and Christian Louboutin have invented different versions of the heel – all men designers scrambled to create a more elevated and much more impractical version of the heel. Designers who are known for creating quality heels usually happen to be men; the irony of men creating such footwear meant especially for women cannot go unnoticed. High heels are known and are proven to be harmful for a woman’s posture and feet, yet there is a persistent buildup of undue pressure on women to continue wearing heels. Christian Louboutin, when asked to comment upon the fact that heels are uncomfortable to walk in and slow a woman down, said, “What is the point of wanting to run?” he continued, “I am all for the pace getting slower, and high heels are very good for that.” That's right, ‘no point of women moving at a fast pace in a man’s world’. According to a study conducted by the American Orthopaedic Foot and Ankle Society, 50% of women experience daily pain from wearing their heels. Heels often put pressure on not only the feet but also the back, spines, knees and toes. High and tightly worn heels also cause blisters for a lot. Aside from this, heels can cause a myriad of other long-term problems as well, such as nerve damage and sciatica. Still, a majority of women continue wearing heels – either by their own choice or otherwise. In most parties or companies, men only feel obligated to wear suits with formal shoes; whereas women feel obligated to wear a dress and high heels as it shows ‘professional etiquette’, and many times dressing professionally can be mistaken with dressing uncomfortably just to please others. Although, women choose to wear heels most of the time as it may hand them a sense of ‘power’ and ‘authority,’ (heels are treated with a sense of hyper feminisation) but in most cases, women feel the need to abide by the invisible aura of the social construct.&amp;nbsp; Nowadays, due to the influence of the fashion industry that has capitalised off of the social construct created by the existence of heels for women, there is a shift or a transition in the way heels have started to be perceived by women in general - earlier, heels used to be treated as modes that were used by a very limited number of people whose usage of heels usually did not allow for free movement or allow for any comfort. Now, due to high-end designers coming up with more innovative heel designs and popularising the usage of heels, women perceive heels to be footwear that, when worn, can be empowering, bold, sexy and fashionable instead of simply being uncomfortable and constrictive.&amp;nbsp; In 2013, Louboutin told Vogue, “It's almost three people who are different in terms of character. I never forget that shoes also have to please men. As a man, I totally understand looking at a girl and saying, “Darling, we're having dinner together tonight, so do you mind to change?” I understand that type of mentality from a man who loves his wife, his woman. He's concerned about the way you look. It's not an ugly thing. I understand also that a woman could not care. I like my design to please women, but also to please men.”&amp;nbsp; His own words make it sound like women are meant to wear heels for ‘the male gaze,’ which seems to be the only somewhat reasonable explanation behind a woman wearing heels. This can be interpreted as women not having to be overly concerned with the way they look while wearing heels, it only matters as long as a man likes what his wife or girlfriend might be wearing.&amp;nbsp; It has become extremely normal to see women wearing extremely high heels in their everyday lives – no matter how unbearable it may be to walk in them. There will most likely not be any shift in society regarding the same, women will continue wearing heels for a long time. What can be normalised though, is ensuring that women need not feel as though they need to adhere to a certain set of expectations from society. The shift from heels being used mostly by men to then being used solely by women is a drastic one; and before we all start feminising heels to such a large extent, we must remember where they originated from and which of the two sexes wore them in the beginning, and also helped in popularising them all around the world. Heels have had a sexist and controlling past, traces of which can be seen in today’s time as well. One can only hope that fashion does not aid in the widening of the gap between men and women. At the end of the day, we must remember that gendered clothing and footwear are not defined by which sex dons the piece the most, but it is defined by how people aside from that specific gender view it. Heels can be worn by any sex or gender, but it happens to be attached to femininity.&amp;nbsp;&amp;nbsp;</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">A Retelling of Childhood</title>
      <link href="/A-Retelling-of-Childhood" rel="alternate" type="text/html" title="A Retelling of Childhood" />
      <published>2020-11-04T00:00:00+00:00</published>
      <updated>2020-11-04T00:00:00+00:00</updated>
      <id>/A-Retelling-of-Childhood</id>
      <content type="html" xml:base="/A-Retelling-of-Childhood">&lt;p&gt;All of us have grown up with stories and tales aplenty - that our parents, grandparents and older siblings used to read to us. I had a little corner in my chest of drawers where I kept my A-3 sized colourfully illustrated books about the &lt;em&gt;Little Mermaid, Beauty and the Beast, Thumbelina, Cinderella&lt;/em&gt; and many others. I would take out one copy of those thin, 15-paged books and ask someone to read them to me or even give it a go at reading myself.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;As a kid, I was fascinated with that world and maybe still haven’t lost my touch with it - because who doesn’t enjoy a good Disney&lt;em&gt; &lt;/em&gt;movie and some hot chocolate to go with it? Disney&lt;em&gt; &lt;/em&gt;is one such institution that has cemented and eternalised the stories that were written hundreds of years ago for children, now told and retold through years, they have changed into a different form - liked by people of all age groups.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;A while back, I came across the writer behind these beloved stories - Hans Christian Anderson. Even though he revolutionised the world of storytelling through his selected works like - &lt;em&gt;The Little Mermaid&lt;/em&gt;,&lt;em&gt; The Snow Queen &lt;/em&gt;(now adapted into the Disney production of &lt;em&gt;Frozen&lt;/em&gt;) and&lt;em&gt; The Ugly Duckling &lt;/em&gt;- his work is criticised for being unfit for children. Now that his work, that dates back to 1835, has been adapted into so many plays, productions and movies, one would think that he used to write perfect fairy tales for a blissful childhood. However, the truth is a bit far from that. Anderson is known for his dark and bitter tales that were written for children but without any filters - just blatant truth and atrocities staring oneself in the face.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Anderson was born and raised in Denmark in a poor family to a shoemaker father and an illiterate washerwoman mother. Still, he persisted with his education at the local school for the poor, and at night used to listen to the tales of the &lt;em&gt;Arabian Nights&lt;/em&gt; from his father. He had a rough go at childhood and had to move out and provide for himself at an early age - the pain of which is reflected in his early-written pieces. The first and original edition of his stories is unlike the retold versions of them in many ways. For starters, his stories were not as happy-go-lucky as we see them now to be. &lt;em&gt;The Little Mermaid&lt;/em&gt;, a classic tale of love and redemption, was actually a disturbing and morbid tale about a mermaid who makes a diabolical bargain with a sea witch and suffers her tongue to be cut out and her tail to be lost, all for the love of a prince. Unfortunately, he completely fails to recognise the enormity of her sacrifice and love for him.&lt;/p&gt;
&lt;p&gt;The Disney film adaption of this tale was morphed into a story about love which always unites, as Ariel and Eric get together in the end, and she leaves her world to be with him. Even though Anderson’s story was bitter, it was realistic because it told you that life doesn’t always give you what or whom you desire, and that it’s not okay to give up your life and the things that make you who you are for anyone - because that never ends well. It reflected that no sacrifice will get you eternal love but maybe patience, time and consent would. Even the tale of &lt;em&gt;Thumbelina&lt;/em&gt; was that of grave pain and a torturous existence, unlike the popular idea.&lt;/p&gt;
&lt;p&gt;I might not have understood that as a kid, but if something is repeatedly told, from a young age then it can have an impact on you in many ways. The debate around protecting kids from bad and traumatic things is acceptable and justified, but only as long as the truth is harmful to them. If not, even kids have a right to be exposed to the right and wrong of life, to disappointment and failure - so that they learn to take something from it instead of thinking of it as the end. A lot of Anderson’s stories were changed for this reason alone. Because his stories did not twist the truth into entangled sentences about a utopian world but told them things that everyone should know - the reality of this harsh world.&lt;/p&gt;
&lt;p&gt;Not only Anderson’s, but many of children’s tales and stories are moulded according to what adults deem right; in fact, some are banned or censored too. A children’s book by the name of &lt;em&gt;And Tango Makes Three&lt;/em&gt; written by Parnell and Richardson was published in 2005, after a real-life incident that occurred in the Central Park Zoo of New York, where two male, chinstrap penguins, Roy and Silo fell in love. The book tells the story of these two male penguins who&lt;/p&gt;
&lt;p&gt;create a family together. With the help of the zookeeper, Mr Gramsay, Roy and Silo are given an egg which they help hatch. The female chick, that completes their family, is consequently named 'Tango' by the zookeepers.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;This book wasn’t received well by many parents, because according to them, homosexuality in animals suggested homosexuality in humans as well. They argued that they weren’t able to explain to their kids as to why “&lt;em&gt;a male penguin has a baby with another male penguin instead of a mommy penguin&lt;/em&gt;”. After retaliation from parents across the world, the book was consequently banned in Singapore, Hong Kong and places across the United States, like Utah, Missouri and Massachusetts.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;ql-cursor&quot;&gt;﻿It is these ingrained notions and our discomfort with the truth that anything different from what we consider ‘normal’, we try to hide from kids instead of having a conversation with them which they might be completely open to.&amp;lt;/p&amp;gt;&lt;p&gt;The question that wavers in my mind is, ‘how much of reality is too much’ to be introduced to children? The reason those stories were changed is that it was considered unfit and inappropriate for a child’s brains. Which might be true from one vantage point, however, why must children be veiled from the realities of the world until a certain age only to be exposed to them suddenly at one go? What is really the right age to see the world in its face?&amp;nbsp;&lt;/p&gt;&lt;p&gt;I’ve always wondered how fair it is to concoct a picture of a world that only disappoints you when you grow up. One, it creates unreal expectations from the world and two, it teaches you things that should be shattered as a concept early on while growing up - be it a princess being saved by the valiant hero at the end, kissing someone without their consent (&lt;em&gt;The Sleeping Beauty&lt;/em&gt;) or having to be perfect for people to like you.&amp;nbsp;&lt;/p&gt;&lt;p&gt;I go back and forth in my mind as to why we think of children as weak or want to shield them from the truth? According to many child psychologists, the impulse to protect kids from unpleasant facts is similar to the self-esteem fad practised by educators. The idea rests on the assumption, not backed by any evidence, that high self-esteem must lead to high academic achievement. Similarly, the instinct to protect children from anything unpleasant is simply out of one’s idea of what a child should and shouldn’t know.&amp;nbsp;&lt;/p&gt;&lt;p&gt;But if one asks themselves, is this reality protecting your child from long-term pain or is just an area of discussion that you don’t want to deal with, or better answer, and so you choose what is convenient - that is, to digress, to lie and to manoeuvre. You leave your children to figure out the jarring and upsetting realities of life by themselves as they grow up. And more often than not, the sources through which your children consume all the things that were hidden from them during their nascent years are not the most reliable or truthful sources. Thus, as we grow up, we are ambushed with facts and to find answers to them we tend to rely on treacherous sources which skew our ideas all the more. Instead of nurturing our kids with righteousness and practicality from the beginning, we try to mask them from facts, for as long as possible - which might not help them as much as one would hope.&amp;nbsp;&lt;/p&gt;&lt;p&gt;The thing about the ‘real’ world is that unlike fairytales, it’s flawed and filled with imperfections. Which is what drives life ahead, the little or large hiccups in the flow of life. From Anderson’s stories to the truth about sexuality, identity, religion and relationships - a lot is made inconspicuous for the kids so that the information dissemination is controlled according to the norms that we have constructed and reaches them in a palatable form. But the question remains: who gets to decide what is right or wrong for these kids? Are flowery stories with morals sieved through our model of heteronormativity, appropriate culture and paradigms of acceptable behaviour the way to go about raising our kids? Only to impel them later into a world where they stumble as they go through life deconstructing their thoughts and knowledge, indoctrinated throughout childhood and then rebuilding it in a frenzy of emotions.&lt;/p&gt;&lt;p&gt;I wonder if it would’ve been better, had my parents read me stories of women who didn’t enforce ideals of self-sacrifice to find the truest form of love or tales of women and people in general, who didn’t settle for anything less than what they deserved, did not change themselves only to be accepted by society, and stories that told that life’s utmost purpose was not to find a perfect partner but to find yourself. And in the process of that, you will meet people who will make life worthwhile. But not one of the stories told us, that it is very much okay, even if you don’t. Because in life, there is no blueprint or algorithm like in stories - all the milestones of life are supposed to make your journey easier and not all the more pressurizing. So the moral of this article is maybe that it is okay to chart your own path - because if you find yourself not fitting in with any of the existing stories, you can always write a new one.&lt;/p&gt;
&lt;/span&gt;&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Poorvi Gupta</name>
        
        
      </author>

      

      
        <category term="society" />
      

      
        <summary type="html">All of us have grown up with stories and tales aplenty - that our parents, grandparents and older siblings used to read to us. I had a little corner in my chest of drawers where I kept my A-3 sized colourfully illustrated books about the Little Mermaid, Beauty and the Beast, Thumbelina, Cinderella and many others. I would take out one copy of those thin, 15-paged books and ask someone to read them to me or even give it a go at reading myself.&amp;nbsp; As a kid, I was fascinated with that world and maybe still haven’t lost my touch with it - because who doesn’t enjoy a good Disney movie and some hot chocolate to go with it? Disney is one such institution that has cemented and eternalised the stories that were written hundreds of years ago for children, now told and retold through years, they have changed into a different form - liked by people of all age groups.&amp;nbsp; A while back, I came across the writer behind these beloved stories - Hans Christian Anderson. Even though he revolutionised the world of storytelling through his selected works like - The Little Mermaid, The Snow Queen (now adapted into the Disney production of Frozen) and The Ugly Duckling - his work is criticised for being unfit for children. Now that his work, that dates back to 1835, has been adapted into so many plays, productions and movies, one would think that he used to write perfect fairy tales for a blissful childhood. However, the truth is a bit far from that. Anderson is known for his dark and bitter tales that were written for children but without any filters - just blatant truth and atrocities staring oneself in the face.&amp;nbsp; Anderson was born and raised in Denmark in a poor family to a shoemaker father and an illiterate washerwoman mother. Still, he persisted with his education at the local school for the poor, and at night used to listen to the tales of the Arabian Nights from his father. He had a rough go at childhood and had to move out and provide for himself at an early age - the pain of which is reflected in his early-written pieces. The first and original edition of his stories is unlike the retold versions of them in many ways. For starters, his stories were not as happy-go-lucky as we see them now to be. The Little Mermaid, a classic tale of love and redemption, was actually a disturbing and morbid tale about a mermaid who makes a diabolical bargain with a sea witch and suffers her tongue to be cut out and her tail to be lost, all for the love of a prince. Unfortunately, he completely fails to recognise the enormity of her sacrifice and love for him. The Disney film adaption of this tale was morphed into a story about love which always unites, as Ariel and Eric get together in the end, and she leaves her world to be with him. Even though Anderson’s story was bitter, it was realistic because it told you that life doesn’t always give you what or whom you desire, and that it’s not okay to give up your life and the things that make you who you are for anyone - because that never ends well. It reflected that no sacrifice will get you eternal love but maybe patience, time and consent would. Even the tale of Thumbelina was that of grave pain and a torturous existence, unlike the popular idea. I might not have understood that as a kid, but if something is repeatedly told, from a young age then it can have an impact on you in many ways. The debate around protecting kids from bad and traumatic things is acceptable and justified, but only as long as the truth is harmful to them. If not, even kids have a right to be exposed to the right and wrong of life, to disappointment and failure - so that they learn to take something from it instead of thinking of it as the end. A lot of Anderson’s stories were changed for this reason alone. Because his stories did not twist the truth into entangled sentences about a utopian world but told them things that everyone should know - the reality of this harsh world. Not only Anderson’s, but many of children’s tales and stories are moulded according to what adults deem right; in fact, some are banned or censored too. A children’s book by the name of And Tango Makes Three written by Parnell and Richardson was published in 2005, after a real-life incident that occurred in the Central Park Zoo of New York, where two male, chinstrap penguins, Roy and Silo fell in love. The book tells the story of these two male penguins who create a family together. With the help of the zookeeper, Mr Gramsay, Roy and Silo are given an egg which they help hatch. The female chick, that completes their family, is consequently named 'Tango' by the zookeepers.&amp;nbsp; This book wasn’t received well by many parents, because according to them, homosexuality in animals suggested homosexuality in humans as well. They argued that they weren’t able to explain to their kids as to why “a male penguin has a baby with another male penguin instead of a mommy penguin”. After retaliation from parents across the world, the book was consequently banned in Singapore, Hong Kong and places across the United States, like Utah, Missouri and Massachusetts. ﻿It is these ingrained notions and our discomfort with the truth that anything different from what we consider ‘normal’, we try to hide from kids instead of having a conversation with them which they might be completely open to.&amp;lt;/p&amp;gt;The question that wavers in my mind is, ‘how much of reality is too much’ to be introduced to children? The reason those stories were changed is that it was considered unfit and inappropriate for a child’s brains. Which might be true from one vantage point, however, why must children be veiled from the realities of the world until a certain age only to be exposed to them suddenly at one go? What is really the right age to see the world in its face?&amp;nbsp;I’ve always wondered how fair it is to concoct a picture of a world that only disappoints you when you grow up. One, it creates unreal expectations from the world and two, it teaches you things that should be shattered as a concept early on while growing up - be it a princess being saved by the valiant hero at the end, kissing someone without their consent (The Sleeping Beauty) or having to be perfect for people to like you.&amp;nbsp;I go back and forth in my mind as to why we think of children as weak or want to shield them from the truth? According to many child psychologists, the impulse to protect kids from unpleasant facts is similar to the self-esteem fad practised by educators. The idea rests on the assumption, not backed by any evidence, that high self-esteem must lead to high academic achievement. Similarly, the instinct to protect children from anything unpleasant is simply out of one’s idea of what a child should and shouldn’t know.&amp;nbsp;But if one asks themselves, is this reality protecting your child from long-term pain or is just an area of discussion that you don’t want to deal with, or better answer, and so you choose what is convenient - that is, to digress, to lie and to manoeuvre. You leave your children to figure out the jarring and upsetting realities of life by themselves as they grow up. And more often than not, the sources through which your children consume all the things that were hidden from them during their nascent years are not the most reliable or truthful sources. Thus, as we grow up, we are ambushed with facts and to find answers to them we tend to rely on treacherous sources which skew our ideas all the more. Instead of nurturing our kids with righteousness and practicality from the beginning, we try to mask them from facts, for as long as possible - which might not help them as much as one would hope.&amp;nbsp;The thing about the ‘real’ world is that unlike fairytales, it’s flawed and filled with imperfections. Which is what drives life ahead, the little or large hiccups in the flow of life. From Anderson’s stories to the truth about sexuality, identity, religion and relationships - a lot is made inconspicuous for the kids so that the information dissemination is controlled according to the norms that we have constructed and reaches them in a palatable form. But the question remains: who gets to decide what is right or wrong for these kids? Are flowery stories with morals sieved through our model of heteronormativity, appropriate culture and paradigms of acceptable behaviour the way to go about raising our kids? Only to impel them later into a world where they stumble as they go through life deconstructing their thoughts and knowledge, indoctrinated throughout childhood and then rebuilding it in a frenzy of emotions.I wonder if it would’ve been better, had my parents read me stories of women who didn’t enforce ideals of self-sacrifice to find the truest form of love or tales of women and people in general, who didn’t settle for anything less than what they deserved, did not change themselves only to be accepted by society, and stories that told that life’s utmost purpose was not to find a perfect partner but to find yourself. And in the process of that, you will meet people who will make life worthwhile. But not one of the stories told us, that it is very much okay, even if you don’t. Because in life, there is no blueprint or algorithm like in stories - all the milestones of life are supposed to make your journey easier and not all the more pressurizing. So the moral of this article is maybe that it is okay to chart your own path - because if you find yourself not fitting in with any of the existing stories, you can always write a new one.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Do People Still Listen to Radio in 2020?: A Decentralised Understanding of Radio Broadcasting in India</title>
      <link href="/Do-People-Still-Listen-to-Radio-in-2020" rel="alternate" type="text/html" title="Do People Still Listen to Radio in 2020?: A Decentralised Understanding of Radio Broadcasting in India " />
      <published>2020-10-31T00:00:00+00:00</published>
      <updated>2020-10-31T00:00:00+00:00</updated>
      <id>/Do-People-Still-Listen-to-Radio-in-2020</id>
      <content type="html" xml:base="/Do-People-Still-Listen-to-Radio-in-2020">&lt;p&gt;Mass media is generally used to disseminate information through various modes of communication. It helps in the exchange of ideas and information to large segments of society. Broadcast is a field of mass communication that transmits information, imparts knowledge and helps in creating a dialogue between the news agency and the audience via mediums like radio and television. The term ‘broadcast’ refers to visual or audio content dispersion to the people through electronic communication in a one-to-many module.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Radio as a Mass Medium&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Known as the ‘theatre of mind’, radio is the most pervasive, accessible and affordable mass media available in the developing world. It networks developing programmes to diverse populations and connects them all on the national and international levels. It is a cheap alternative to the glamourous mediums that have evolved with technology. The radio still remains a potential driver of information and entertainment to highly targeted audiences such as the rural population. Especially in a developing country, like India, this plays a major role in spreading information because most of the rural population is generally left bereft of the newer mechanisms.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Evolution of Radio&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The objective of the first radio program in India which was transmitted by the Madras Presidency Radio Club (MPRC) and led by C V Krishnaswami Cheti, was to stimulate interest, inform and instruct the public, and to foster the study of radio communications in the presidency. With the approval being given for the privatisation of radio media organisations by the British colonial government, the Indian Broadcasting Company set up radio stations in Calcutta and Bombay. In 1936, this corporation was renamed All India Radio (AIR), and when India became independent in 1947, AIR became a separate department under the Ministry of Information and Broadcasting. Presently, radio broadcasting gets coverage in 24 languages and 146 dialects all across India. 111 million households own a radio set in the country. Regional radio has reached all states individually, catering to its colloquial audience.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://d2r2ijn7njrktv.cloudfront.net/IL/uploads/2017/02/community-radio.jpg&quot; rel=&quot;noopener noreferrer&quot; target=&quot;_blank&quot;&gt;&lt;div class=&quot;caption&quot; style=&quot;text-align: left&quot;&gt;&lt;img src=&quot;assets/images/community-radio.jpg&quot; /&gt;Community Radio stations are popular among all age groups and used extensively to spread awareness.&lt;/div&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Local Broadcasting: Community Radio&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;It refers to a type of radio service that caters to the interests of a particular person in an area. The primary focus is on the local population and issues concerning them. Use of broadcast here acts as a link between the community problems and the authorities who can do something about the resolution of the same. Community radio is a broadcast tool that is of the people, by the people and for the people. It is a decentralised use of the larger medium of communication, that is, the radio. It is important to talk about community radio in today’s times as it provides a platform to the local people to become citizen journalists and educate themselves and their fellow beings. While FM Radio caters to the urban population mostly, community radio’s target audience includes rural, urban, suburban, regional and sectional communities.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Radio Udaan &lt;/strong&gt;is an example of community radio. Launched in 2014, it is an online radio station developed by and for specially-abled people. &amp;nbsp;It is the only type of radio station that is run by visually challenged people and is widely listened to in more than 115 countries. The vision of the radio station is broad and its agendas are diverse, which is- “&lt;em&gt;To entertain, empower and elevate the listeners. Radio Udaan aims to reach out to maximum possible people. It wants to become not only a supplier of entertainment and knowledge to the listeners but also a part of their daily routine, family and lifestyle. The station seeks to shoulder the burdens of its listeners by empowering them and listening to their stories and problems, and eventually catering them solutions&lt;/em&gt;.”&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Apno Radio, 90.4 MHz, Rajasthan &lt;/strong&gt;was launched in 2005 and is Rajasthan’s first community radio station. The station’s objective is to promote self-reliance and women's empowerment. It focuses on several social issues ailing the rural communities of the state such as education, health, nutrition, environment, agriculture, depicting folk, art, culture, rural and community development.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Is Radio Dying?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;While the millennials in urban life devote their maximum time to Apple Music and Spotify, this question often pops up with “yes” as an answer to it. However, that is not true. Radio is not dying but rather it is evolving. The term ‘Tradigital’ (traditional plus digital) is assigned to this evolution wherein traditional radio stations are using digital media platforms for two reasons - one, to expand their audience reach and two, to keep up with the technology and not let the broadcasting medium which has a reach of over 100 million people in India die because of the takeover by bigger and international online streaming platforms, television and satellite revolution. The newer versions of mass communications generally caters to a literate population with access to higher-end devices and it must be acknowledged that radio initially started to educate the uneducated and still plays a pivotal role in doing this duty as a medium of communication. Recognition of local broadcasting is crucial to its development and as we talk more about ‘Communication as a Development Source’ in the 21st century, community reach can boost the country’s process to achieve 2030 Sustainable Development Goals by imparting knowledge in a decentralised manner to the regional sections. Moreover, radio acts as a tool for both development and freedom which provides a platform for people to exercise their freedom of speech and expression, uphold the liberty of thought, especially with the growth of community radio.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;To understand this better, I would like to connect this with a media theory called the Knowledge Gap Theory of 1970. It is the brainchild of Philip J Tichenor, George A Donohue and Clarice N Olien. They explained the Knowledge Gap theory as “&lt;em&gt;the infusion of mass media information into a social system where higher socioeconomic status segments tend to acquire this information faster than lower socioeconomic status population segments. Hence&amp;nbsp;the&amp;nbsp;gap between the two tends to increase rather than decrease.&lt;/em&gt;” In 2020, these faster modes of communication can be through the accessibility of various sources of information. The availability of these mediums come with a price tag and not everyone can afford it. Radio and especially the growing community radio has revolutionised news delivery in various parts of the country that do not get the proper mainstream coverage in the primetime and are highly underrepresented and neglected in the little coverage that they receive. The only way to connect to the grassroots is by reducing this knowledge gap with a tool of communication. This is where the power of radio lies, in the heart of the community people who use this broadcasting medium to connect and embrace positive change in their society. A boost to a media platform which caters to the lower rungs of the socio-economic ladder is necessary in these times when inequality and unemployment are raging and the youth is being fed with narratives filled with hatred and bigotry. This medium can be the harbinger of change in the society for the better, we just have to care enough about the people left behind to dedicate ourselves to such discourse. &lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Himanshi Saini</name>
        
        
      </author>

      

      
        <category term="society" />
      

      
        <summary type="html">Mass media is generally used to disseminate information through various modes of communication. It helps in the exchange of ideas and information to large segments of society. Broadcast is a field of mass communication that transmits information, imparts knowledge and helps in creating a dialogue between the news agency and the audience via mediums like radio and television. The term ‘broadcast’ refers to visual or audio content dispersion to the people through electronic communication in a one-to-many module.&amp;nbsp; Radio as a Mass Medium Known as the ‘theatre of mind’, radio is the most pervasive, accessible and affordable mass media available in the developing world. It networks developing programmes to diverse populations and connects them all on the national and international levels. It is a cheap alternative to the glamourous mediums that have evolved with technology. The radio still remains a potential driver of information and entertainment to highly targeted audiences such as the rural population. Especially in a developing country, like India, this plays a major role in spreading information because most of the rural population is generally left bereft of the newer mechanisms.&amp;nbsp; Evolution of Radio The objective of the first radio program in India which was transmitted by the Madras Presidency Radio Club (MPRC) and led by C V Krishnaswami Cheti, was to stimulate interest, inform and instruct the public, and to foster the study of radio communications in the presidency. With the approval being given for the privatisation of radio media organisations by the British colonial government, the Indian Broadcasting Company set up radio stations in Calcutta and Bombay. In 1936, this corporation was renamed All India Radio (AIR), and when India became independent in 1947, AIR became a separate department under the Ministry of Information and Broadcasting. Presently, radio broadcasting gets coverage in 24 languages and 146 dialects all across India. 111 million households own a radio set in the country. Regional radio has reached all states individually, catering to its colloquial audience. Community Radio stations are popular among all age groups and used extensively to spread awareness. Local Broadcasting: Community Radio&amp;nbsp; It refers to a type of radio service that caters to the interests of a particular person in an area. The primary focus is on the local population and issues concerning them. Use of broadcast here acts as a link between the community problems and the authorities who can do something about the resolution of the same. Community radio is a broadcast tool that is of the people, by the people and for the people. It is a decentralised use of the larger medium of communication, that is, the radio. It is important to talk about community radio in today’s times as it provides a platform to the local people to become citizen journalists and educate themselves and their fellow beings. While FM Radio caters to the urban population mostly, community radio’s target audience includes rural, urban, suburban, regional and sectional communities.&amp;nbsp; Radio Udaan is an example of community radio. Launched in 2014, it is an online radio station developed by and for specially-abled people. &amp;nbsp;It is the only type of radio station that is run by visually challenged people and is widely listened to in more than 115 countries. The vision of the radio station is broad and its agendas are diverse, which is- “To entertain, empower and elevate the listeners. Radio Udaan aims to reach out to maximum possible people. It wants to become not only a supplier of entertainment and knowledge to the listeners but also a part of their daily routine, family and lifestyle. The station seeks to shoulder the burdens of its listeners by empowering them and listening to their stories and problems, and eventually catering them solutions.” Apno Radio, 90.4 MHz, Rajasthan was launched in 2005 and is Rajasthan’s first community radio station. The station’s objective is to promote self-reliance and women's empowerment. It focuses on several social issues ailing the rural communities of the state such as education, health, nutrition, environment, agriculture, depicting folk, art, culture, rural and community development.&amp;nbsp; Is Radio Dying? While the millennials in urban life devote their maximum time to Apple Music and Spotify, this question often pops up with “yes” as an answer to it. However, that is not true. Radio is not dying but rather it is evolving. The term ‘Tradigital’ (traditional plus digital) is assigned to this evolution wherein traditional radio stations are using digital media platforms for two reasons - one, to expand their audience reach and two, to keep up with the technology and not let the broadcasting medium which has a reach of over 100 million people in India die because of the takeover by bigger and international online streaming platforms, television and satellite revolution. The newer versions of mass communications generally caters to a literate population with access to higher-end devices and it must be acknowledged that radio initially started to educate the uneducated and still plays a pivotal role in doing this duty as a medium of communication. Recognition of local broadcasting is crucial to its development and as we talk more about ‘Communication as a Development Source’ in the 21st century, community reach can boost the country’s process to achieve 2030 Sustainable Development Goals by imparting knowledge in a decentralised manner to the regional sections. Moreover, radio acts as a tool for both development and freedom which provides a platform for people to exercise their freedom of speech and expression, uphold the liberty of thought, especially with the growth of community radio.&amp;nbsp; To understand this better, I would like to connect this with a media theory called the Knowledge Gap Theory of 1970. It is the brainchild of Philip J Tichenor, George A Donohue and Clarice N Olien. They explained the Knowledge Gap theory as “the infusion of mass media information into a social system where higher socioeconomic status segments tend to acquire this information faster than lower socioeconomic status population segments. Hence&amp;nbsp;the&amp;nbsp;gap between the two tends to increase rather than decrease.” In 2020, these faster modes of communication can be through the accessibility of various sources of information. The availability of these mediums come with a price tag and not everyone can afford it. Radio and especially the growing community radio has revolutionised news delivery in various parts of the country that do not get the proper mainstream coverage in the primetime and are highly underrepresented and neglected in the little coverage that they receive. The only way to connect to the grassroots is by reducing this knowledge gap with a tool of communication. This is where the power of radio lies, in the heart of the community people who use this broadcasting medium to connect and embrace positive change in their society. A boost to a media platform which caters to the lower rungs of the socio-economic ladder is necessary in these times when inequality and unemployment are raging and the youth is being fed with narratives filled with hatred and bigotry. This medium can be the harbinger of change in the society for the better, we just have to care enough about the people left behind to dedicate ourselves to such discourse.</summary>
      

      
      
    </entry>
  
</feed>
